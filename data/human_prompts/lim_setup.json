{
  "llm_config": {
    "gemini_project_id": "YOUR_PROJECT_ID",
    "gemini_location": "us-central1",
    "model_judge": "gemini-1.5-flash",
    "model_readme": "gemini-1.5-flash",
    "temperature_judge": 0.2,
    "temperature_readme": 0.5,
    "max_output_tokens": 2048
  },
  "prompts": {
    "judge_system": "You are a model that evaluates user satisfaction with a README.md based ONLY on nonverbal behavior summaries.\\n\\nInput: a JSON object that contains:\\n- metadata, context: basic info about the session and which README/section was shown.\\n- emotion_stats: average probabilities of facial emotions over the window.\\n- pose_stats: head and body movement stats (variance in nose position, shoulder z-difference, etc.).\\n- behavior_analysis: categorical interpretations (facial_expression, head_gesture, body_posture).\\n- final_interpretation_for_llm: a short rule-based textual summary of the user's nonverbal reaction.\\n\\nYou must:\\n1) Infer how satisfied the user likely was while reading this README section on a 0.0–1.0 scale.\\n2) Use four anchor emotions as an internal axis for the satisfaction score:\\n   - Satisfaction (positive end)\\n   - Surprise (modulator; neutral on its own, positive or negative depending on context)\\n   - Dissatisfaction (soft negative)\\n   - Annoyance (strong negative)\\n3) Output a single JSON with this EXACT structure:\\n\\n{\\n  \"overall_satisfaction\": {\\n    \"score\": <float 0.0-1.0>,\\n    \"level\": \"very_low\" | \"low\" | \"medium\" | \"high\" | \"very_high\",\\n    \"confidence\": <float 0.0-1.0>\\n  },\\n  \"anchors\": {\\n    \"satisfaction\": {\\n      \"intensity\": 0 | 1 | 2 | 3,\\n      \"confidence\": <float>,\\n      \"evidence\": \"<short explanation>\"\\n    },\\n    \"surprise\": {\\n      \"intensity\": 0 | 1 | 2 | 3,\\n      \"polarity\": \"neutral\" | \"positive\" | \"negative\",\\n      \"confidence\": <float>,\\n      \"evidence\": \"<short explanation>\"\\n    },\\n    \"dissatisfaction\": {\\n      \"intensity\": 0 | 1 | 2 | 3,\\n      \"confidence\": <float>,\\n      \"evidence\": \"<short explanation>\"\\n    },\\n    \"annoyance\": {\\n      \"intensity\": 0 | 1 | 2 | 3,\\n      \"confidence\": <float>,\\n      \"evidence\": \"<short explanation>\"\\n    }\\n  },\\n  \"readme_evaluation\": {\\n    \"dimensions\": {\\n      \"clarity\": { \"score\": <float 0.0-1.0>, \"comment\": \"\" },\\n      \"structure\": { \"score\": <float 0.0-1.0>, \"comment\": \"\" },\\n      \"length\": { \"score\": <float 0.0-1.0>, \"comment\": \"\" },\\n      \"examples\": { \"score\": <float 0.0-1.0>, \"comment\": \"\" },\\n      \"visual_hierarchy\": { \"score\": <float 0.0-1.0>, \"comment\": \"\" },\\n      \"onboarding_friendliness\": { \"score\": <float 0.0-1.0>, \"comment\": \"\" }\\n    },\\n    \"strong_points\": [\"<bullet text>\"],\\n    \"weak_points\": [\"<bullet text>\"]\\n  },\\n  \"generation_prompt\": {\\n    \"short_hint\": \"<one-line hint to improve the README>\",\\n    \"full_prompt_for_next_readme\": \"<prompt in English that instructs another LLM how to rewrite the README to improve user satisfaction based on the observations>\"\\n  }\\n}\\n\\nMapping rules:\\n- Higher Happiness and Engagement (leaning forward, nodding) → higher Satisfaction anchor and higher overall_satisfaction.score.\\n- Confusion signals (head shaking, Confused / Surprised context) → higher Surprise with negative polarity and/or higher Dissatisfaction.\\n- Strong restless movement, leaning backward with negative emotions → higher Annoyance and lower satisfaction.\\n- Use behavior_analysis and final_interpretation_for_llm as hints, but also consider the numeric stats in emotion_stats and pose_stats.\\n\\nVERY IMPORTANT:\\n- Output must be VALID JSON.\\n- Do NOT output any text outside the JSON.\\n- Do NOT add or remove fields. Use exactly the schema above.",
    "judge_user_template": "Below is a JSON summary of a single interaction window while a user was reading a README section. Analyze it and produce the required output JSON.\\n\\n```json\\n{json_payload}\\n```",
    "readme_generator_system": "You are an AI assistant that writes and rewrites README.md files to maximize user satisfaction, clarity, and onboarding experience.\\n\\nYou receive:\\n- Project metadata (what the project does, tech stack, main features).\\n- The previous version of the README.md (if any).\\n- Feedback derived from real user nonverbal reactions, packaged as a natural language prompt from another LLM (`generation_prompt.full_prompt_for_next_readme`).\\n\\nYour task:\\n- Generate a NEW README.md that keeps what worked well and improves what caused confusion, dissatisfaction, or annoyance.\\n- Follow all instructions described in the feedback prompt.\\n- Use clear Markdown structure: title, overview, installation, quick start, examples, configuration, etc., as appropriate.\\n- Prefer concise language, bullet points, and code examples over long paragraphs.\\n\\nOutput: only the README.md content in Markdown, no additional commentary.",
    "readme_generator_user_template": "Project metadata:\\n\\n{project_metadata}\\n\\nPrevious README (may be empty):\\n\\n```markdown\\n{previous_readme}\\n```\\n\\nUser reaction-based feedback for the next README revision:\\n\\n\"\"\"\\n{generation_prompt_full}\\n\"\"\"\\n\\nNow write the new README.md."
  },
  "code": {
    "python": {
      "judge_call_example": "import json\\nfrom google import genai\\n\\nCONFIG = {\\n    \"project_id\": \"YOUR_PROJECT_ID\",\\n    \"location\": \"us-central1\",\\n    \"model\": \"gemini-1.5-flash\"\\n}\\n\\nclient = genai.Client()\\n\\n# 이 문자열은 llm_setup.json의 prompts.judge_system 에서 불러온다고 가정\\nJUDGE_SYSTEM_PROMPT = \"...\"  # load from JSON\\nJUDGE_USER_TEMPLATE = \"...\"   # load from JSON\\n\\n\\ndef call_judge_llm(analysis_json: dict) -> dict:\\n    \\\"\\\"\\\"\\n    analysis_json: 전처리된 *_analysis.json (metadata, context, emotion_stats, pose_stats, behavior_analysis, final_interpretation_for_llm)\\n    반환값: 평가 LLM이 만든 JSON (overall_satisfaction, anchors, readme_evaluation, generation_prompt)\\n    \\\"\\\"\\\"\\n    user_text = JUDGE_USER_TEMPLATE.replace(\"{json_payload}\", json.dumps(analysis_json, ensure_ascii=False, indent=2))\\n\\n    response = client.models.generate_content(\\n        model=CONFIG[\"model\"],\\n        contents=[\\n            {\"role\": \"system\", \"parts\": [{\"text\": JUDGE_SYSTEM_PROMPT}]},\\n            {\"role\": \"user\", \"parts\": [{\"text\": user_text}]}\\n        ],\\n        generation_config={\\n            \"temperature\": 0.2,\\n            \"max_output_tokens\": 2048\\n        }\\n    )\\n\\n    text = response.text\\n    return json.loads(text)\\n",
      "generate_readme_example": "import json\\nfrom google import genai\\n\\nCONFIG = {\\n    \"project_id\": \"YOUR_PROJECT_ID\",\\n    \"location\": \"us-central1\",\\n    \"model\": \"gemini-1.5-flash\"\\n}\\n\\nclient = genai.Client()\\n\\nREADME_SYSTEM_PROMPT = \"...\"           # prompts.readme_generator_system\\nREADME_USER_TEMPLATE = \"...\"            # prompts.readme_generator_user_template\\n\\n\\ndef call_readme_generator(project_metadata: str, previous_readme: str, generation_prompt_full: str) -> str:\\n    user_text = README_USER_TEMPLATE\\n    user_text = user_text.replace(\"{project_metadata}\", project_metadata)\\n    user_text = user_text.replace(\"{previous_readme}\", previous_readme)\\n    user_text = user_text.replace(\"{generation_prompt_full}\", generation_prompt_full)\\n\\n    response = client.models.generate_content(\\n        model=CONFIG[\"model\"],\\n        contents=[\\n            {\"role\": \"system\", \"parts\": [{\"text\": README_SYSTEM_PROMPT}]},\\n            {\"role\": \"user\", \"parts\": [{\"text\": user_text}]}\\n        ],\\n        generation_config={\\n            \"temperature\": 0.5,\\n            \"max_output_tokens\": 4096\\n        }\\n    )\\n\\n    return response.text\\n",
      "full_pipeline_example": "import json\\nimport glob\\nimport os\\nfrom google import genai\\n\\n# 1) 설정 로드 (이 파일 자체가 지금 보고 있는 llm_setup.json 이라고 가정)\\nwith open(\"llm_setup.json\", \"r\", encoding=\"utf-8\") as f:\\n    LLM_SETUP = json.load(f)\\n\\nCONFIG = {\\n    \"project_id\": LLM_SETUP[\"llm_config\"].get(\"gemini_project_id\", \"YOUR_PROJECT_ID\"),\\n    \"location\": LLM_SETUP[\"llm_config\"].get(\"gemini_location\", \"us-central1\"),\\n    \"model_judge\": LLM_SETUP[\"llm_config\"].get(\"model_judge\", \"gemini-1.5-flash\"),\\n    \"model_readme\": LLM_SETUP[\"llm_config\"].get(\"model_readme\", \"gemini-1.5-flash\")\\n}\\n\\nclient = genai.Client()\\n\\nJUDGE_SYSTEM_PROMPT = LLM_SETUP[\"prompts\"][\"judge_system\"]\\nJUDGE_USER_TEMPLATE = LLM_SETUP[\"prompts\"][\"judge_user_template\"]\\nREADME_SYSTEM_PROMPT = LLM_SETUP[\"prompts\"][\"readme_generator_system\"]\\nREADME_USER_TEMPLATE = LLM_SETUP[\"prompts\"][\"readme_generator_user_template\"]\\n\\n\\ndef call_judge_llm(analysis_json: dict) -> dict:\\n    user_text = JUDGE_USER_TEMPLATE.replace(\\n        \"{json_payload}\", json.dumps(analysis_json, ensure_ascii=False, indent=2)\\n    )\\n\\n    response = client.models.generate_content(\\n        model=CONFIG[\"model_judge\"],\\n        contents=[\\n            {\"role\": \"system\", \"parts\": [{\"text\": JUDGE_SYSTEM_PROMPT}]},\\n            {\"role\": \"user\", \"parts\": [{\"text\": user_text}]}\\n        ],\\n        generation_config={\\n            \"temperature\": LLM_SETUP[\"llm_config\"].get(\"temperature_judge\", 0.2),\\n            \"max_output_tokens\": LLM_SETUP[\"llm_config\"].get(\"max_output_tokens\", 2048)\\n        }\\n    )\\n    return json.loads(response.text)\\n\\n\\ndef call_readme_generator(project_metadata: str, previous_readme: str, generation_prompt_full: str) -> str:\\n    user_text = README_USER_TEMPLATE\\n    user_text = user_text.replace(\"{project_metadata}\", project_metadata)\\n    user_text = user_text.replace(\"{previous_readme}\", previous_readme)\\n    user_text = user_text.replace(\"{generation_prompt_full}\", generation_prompt_full)\\n\\n    response = client.models.generate_content(\\n        model=CONFIG[\"model_readme\"],\\n        contents=[\\n            {\"role\": \"system\", \"parts\": [{\"text\": README_SYSTEM_PROMPT}]},\\n            {\"role\": \"user\", \"parts\": [{\"text\": user_text}]}\\n        ],\\n        generation_config={\\n            \"temperature\": LLM_SETUP[\"llm_config\"].get(\"temperature_readme\", 0.5),\\n            \"max_output_tokens\": LLM_SETUP[\"llm_config\"].get(\"max_output_tokens\", 4096)\\n        }\\n    )\\n    return response.text\\n\\n\\ndef run_one_iteration(analysis_json_path: str, project_metadata: str, previous_readme_path: str, out_readme_path: str):\\n    # 1) 전처리된 비언어 정보 로드\\n    with open(analysis_json_path, \"r\", encoding=\"utf-8\") as f:\\n        analysis_json = json.load(f)\\n\\n    # 2) 만족도 LLM 호출\\n    judge_output = call_judge_llm(analysis_json)\\n    generation_prompt_full = judge_output[\"generation_prompt\"][\"full_prompt_for_next_readme\"]\\n\\n    # 3) 이전 README 로드 (없으면 빈 문자열)\\n    if os.path.exists(previous_readme_path):\\n        with open(previous_readme_path, \"r\", encoding=\"utf-8\") as f:\\n            previous_readme = f.read()\\n    else:\\n        previous_readme = \"\"\\n\\n    # 4) README 생성 LLM 호출\\n    new_readme = call_readme_generator(project_metadata, previous_readme, generation_prompt_full)\\n\\n    # 5) 새 README 저장\\n    with open(out_readme_path, \"w\", encoding=\"utf-8\") as f:\\n        f.write(new_readme)\\n\\n    return judge_output, new_readme\\n\\n\\nif __name__ == \"__main__\":\\n    # 예시 실행\\n    analysis_json_path = \"preprocessed/emotion_log_XXXX_analysis.json\"\\n    project_metadata = \"\"\"\\nName: My README Optimization Project\\nDescription: A system that auto-generates and optimizes README.md using user nonverbal reactions.\\nTech stack: Python, OpenCV, MediaPipe, EmotiEffLib, Gemini API.\\n\"\"\"\\n    previous_readme_path = \"README_prev.md\"\\n    out_readme_path = \"README_next.md\"\\n\\n    judge_output, new_readme = run_one_iteration(\\n        analysis_json_path, project_metadata, previous_readme_path, out_readme_path\\n    )\\n\\n    print(\"Overall satisfaction:\", judge_output[\"overall_satisfaction\"])\\n    print(\"Short hint:\", judge_output[\"generation_prompt\"][\"short_hint\"])\\n"
    }
  }
}
