{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8416e147-536e-4cbd-8675-51f330eb8804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\OSS\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.9.1+cu126\n",
      "cuda: True / NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "cv2: 4.11.0\n",
      "timm: 0.9.12\n",
      "models: ['enet_b0_8_best_vgaf', 'enet_b0_8_best_afew', 'enet_b2_8', 'enet_b0_8_va_mtl', 'enet_b2_7']\n",
      "Downloading enet_b0_8_best_vgaf.pt from https://github.com/sb-ai-lab/EmotiEffLib/blob/main/models/affectnet_emotions/enet_b0_8_best_vgaf.pt?raw=true\n",
      "✅ EmotiEffLib + MTCNN 초기화 완료\n"
     ]
    }
   ],
   "source": [
    "import torch, cv2, facenet_pytorch, timm, emotiefflib, numpy as np\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda:\", torch.cuda.is_available(), \"/\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"no gpu\")\n",
    "print(\"cv2:\", cv2.__version__)\n",
    "print(\"timm:\", timm.__version__)\n",
    "\n",
    "from emotiefflib.facial_analysis import EmotiEffLibRecognizer, get_model_list\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "print(\"models:\", get_model_list()[:5])\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "rec = EmotiEffLibRecognizer(engine=\"torch\", model_name=\"enet_b0_8_best_vgaf\", device=device)\n",
    "\n",
    "print(\"✅ EmotiEffLib + MTCNN 초기화 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a04232a-65b2-4c77-b677-9bbcb73d5905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "[INFO] Available models: ['enet_b0_8_best_vgaf', 'enet_b0_8_best_afew', 'enet_b2_8', 'enet_b0_8_va_mtl', 'enet_b2_7', 'mbf_va_mtl', 'mobilevit_va_mtl']\n",
      "[INFO] EmotiEffLib model loaded: enet_b0_8_best_vgaf\n",
      "[INFO] Press 'q' to quit\n",
      "['Neutral'] [[1.1882356e-01 3.1404372e-02 3.9325215e-04 2.4907892e-03 1.4925869e-03\n",
      "  4.9974835e-01 3.4211966e-01 3.5274166e-03]]\n",
      "['Neutral'] [[0.15038133 0.02921563 0.00069888 0.00137091 0.00207686 0.4828465\n",
      "  0.32904074 0.00436922]]\n",
      "['Neutral'] [[1.0987236e-01 3.3147104e-02 1.9615873e-04 1.8376418e-03 1.3343418e-03\n",
      "  5.3034306e-01 3.2077417e-01 2.4951203e-03]]\n",
      "['Neutral'] [[1.3325629e-01 2.7485231e-02 4.7035387e-04 2.5909957e-03 1.3393170e-03\n",
      "  4.8330066e-01 3.4812614e-01 3.4310631e-03]]\n",
      "['Neutral'] [[0.10126219 0.02902524 0.00050957 0.0035685  0.00170875 0.48656476\n",
      "  0.37278518 0.00457577]]\n",
      "['Neutral'] [[9.3358666e-02 3.0664444e-02 2.2952790e-04 2.1803258e-03 1.5964846e-03\n",
      "  5.7867676e-01 2.8949457e-01 3.7991896e-03]]\n",
      "['Neutral'] [[7.1730435e-02 1.8688047e-02 2.0025909e-04 1.6027780e-03 9.8882860e-04\n",
      "  5.1405275e-01 3.8855374e-01 4.1831662e-03]]\n",
      "['Neutral'] [[0.13663715 0.03440193 0.00065366 0.00364819 0.0016675  0.47136608\n",
      "  0.3482482  0.0033774 ]]\n",
      "['Neutral'] [[0.12824538 0.03834159 0.00105873 0.0020766  0.0022041  0.44409293\n",
      "  0.3797911  0.00418955]]\n",
      "['Neutral'] [[0.10633169 0.0312104  0.00069038 0.00282896 0.00181154 0.49362925\n",
      "  0.35756633 0.00593148]]\n",
      "['Neutral'] [[0.12501965 0.03743805 0.00067836 0.00362139 0.00231978 0.49102738\n",
      "  0.33262858 0.00726678]]\n",
      "['Neutral'] [[0.17255388 0.04987245 0.00090069 0.00261206 0.00284862 0.49688846\n",
      "  0.26988035 0.00444348]]\n",
      "['Sadness'] [[9.6228741e-02 1.6796893e-02 1.4830020e-04 1.4383311e-03 7.9806568e-04\n",
      "  4.2136449e-01 4.5958352e-01 3.6416806e-03]]\n",
      "['Sadness'] [[0.12202807 0.01395851 0.00059563 0.00273502 0.00066288 0.3784309\n",
      "  0.47807157 0.00351744]]\n",
      "['Neutral'] [[1.4472727e-01 3.0139444e-02 4.2686911e-04 2.7124786e-03 1.1711628e-03\n",
      "  4.5351514e-01 3.6262214e-01 4.6855160e-03]]\n",
      "['Sadness'] [[3.8970366e-02 7.5967754e-03 2.5246674e-04 4.6282308e-03 5.3051842e-04\n",
      "  3.5101724e-01 5.9028345e-01 6.7209122e-03]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from facenet_pytorch import MTCNN\n",
    "from emotiefflib.facial_analysis import EmotiEffLibRecognizer, get_model_list\n",
    "\n",
    "def main():\n",
    "    # 1. 디바이스 설정\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"[INFO] Using device:\", device)\n",
    "\n",
    "    # 2. 모델 리스트 확인 (옵션)\n",
    "    print(\"[INFO] Available models:\", get_model_list())\n",
    "    model_name = \"enet_b0_8_best_vgaf\"  # 8-class 감정 모델\n",
    "\n",
    "    # 3. MTCNN (얼굴 검출기) + EmotiEffLib (감정 인식기)\n",
    "    mtcnn = MTCNN(keep_all=True, device=device)\n",
    "\n",
    "    # 공식 문서 기준: engine=\"torch\", model_name, device 세 개만 넘김\n",
    "    emotion_recognizer = EmotiEffLibRecognizer(\n",
    "        engine=\"torch\",\n",
    "        model_name=model_name,\n",
    "        device=device,\n",
    "    )\n",
    "    print(\"[INFO] EmotiEffLib model loaded:\", model_name)\n",
    "    print(\"[INFO] Press 'q' to quit\")\n",
    "\n",
    "    # 4. 웹캠 열기\n",
    "    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "    if not cap.isOpened():\n",
    "        print(\"[ERROR] Cannot open webcam\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"[ERROR] Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        # BGR(OpenCV) → RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # 4-1. 얼굴 bbox 검출\n",
    "        boxes, _ = mtcnn.detect(frame_rgb)\n",
    "\n",
    "        emotions = []\n",
    "        facial_images_np = []\n",
    "\n",
    "        if boxes is not None:\n",
    "            # 4-2. 얼굴 텐서 추출 (MTCNN 공식 사용법)\n",
    "            # keep_all=True 이므로 faces는 (N, 3, H, W) 텐서일 가능성이 큼\n",
    "            faces = mtcnn(frame_rgb)\n",
    "\n",
    "            if faces is not None:\n",
    "                # faces가 torch.Tensor인 경우와 리스트인 경우를 모두 처리\n",
    "                if isinstance(faces, torch.Tensor):\n",
    "                    if faces.ndim == 3:  # (3, H, W) → 1개 얼굴\n",
    "                        faces = faces.unsqueeze(0)  # (1, 3, H, W)\n",
    "                    faces_iter = [f for f in faces]\n",
    "                else:\n",
    "                    # 이미 리스트/튜플이면 그대로\n",
    "                    faces_iter = list(faces)\n",
    "\n",
    "                # 4-3. 얼굴 텐서를 numpy(H, W, C, uint8)로 변환\n",
    "                for face in faces_iter:\n",
    "                    face_np = face.permute(1, 2, 0).cpu().numpy()      # (H, W, C), float32, [-1, 1] 근처\n",
    "                    face_np = (face_np * 128 + 127.5).clip(0, 255).astype(np.uint8)\n",
    "                    facial_images_np.append(face_np)\n",
    "\n",
    "                if len(facial_images_np) > 0:\n",
    "                    try:\n",
    "                        # EmotiEffLib 공식 문서: (H, W, C) RGB np.ndarray 리스트를 넣어줌\n",
    "                        emotions, _ = emotion_recognizer.predict_emotions(\n",
    "                            facial_images_np,\n",
    "                            logits=True,  # 확률만 원하면 False로\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(\"[ERROR] Emotion prediction failed:\", e)\n",
    "                        emotions = [\"error\"] * len(facial_images_np)\n",
    "            else:\n",
    "                boxes = None  # faces 없으면 boxes도 같이 무시\n",
    "        print(emotions, _)\n",
    "        # 4-4. bbox + 감정 라벨 그리기\n",
    "        if boxes is not None and emotions:\n",
    "            for box, emotion in zip(boxes, emotions):\n",
    "                if box is None:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = [int(b) for b in box]\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    emotion,\n",
    "                    (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.7,\n",
    "                    (0, 255, 0),\n",
    "                    2,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "        else:\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                \"No face detected\",\n",
    "                (20, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                1.0,\n",
    "                (0, 0, 255),\n",
    "                2,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "\n",
    "        cv2.imshow(\"EmotiEffLib Webcam Emotion Recognition (press 'q' to quit)\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7aef279-ba4a-4d20-89c0-35d61cdbe830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "[INFO] Available models: ['enet_b0_8_best_vgaf', 'enet_b0_8_best_afew', 'enet_b2_8', 'enet_b0_8_va_mtl', 'enet_b2_7', 'mbf_va_mtl', 'mobilevit_va_mtl']\n",
      "[INFO] EmotiEffLib model loaded: enet_b0_8_best_vgaf\n",
      "[INFO] Press 'q' to quit\n",
      "=== Emotion probabilities (Face 0) ===\n",
      "Anger    :   1.3%\n",
      "Contempt :   0.2%\n",
      "Disgust  :   0.0%\n",
      "Fear     :   0.3%\n",
      "Happiness:   0.0%\n",
      "Neutral  :  42.1%\n",
      "Sadness  :  54.9%\n",
      "Surprise :   1.2%\n",
      "======================================\n",
      "=== Emotion probabilities (Face 0) ===\n",
      "Anger    :   2.0%\n",
      "Contempt :   1.5%\n",
      "Disgust  :   0.0%\n",
      "Fear     :   1.1%\n",
      "Happiness:   0.2%\n",
      "Neutral  :  62.7%\n",
      "Sadness  :  25.5%\n",
      "Surprise :   6.9%\n",
      "======================================\n",
      "=== Emotion probabilities (Face 0) ===\n",
      "Anger    :   2.1%\n",
      "Contempt :   2.5%\n",
      "Disgust  :   0.0%\n",
      "Fear     :   0.3%\n",
      "Happiness:   0.4%\n",
      "Neutral  :  70.4%\n",
      "Sadness  :  22.7%\n",
      "Surprise :   1.5%\n",
      "======================================\n",
      "=== Emotion probabilities (Face 0) ===\n",
      "Anger    :   2.9%\n",
      "Contempt :   8.0%\n",
      "Disgust  :   0.0%\n",
      "Fear     :   0.2%\n",
      "Happiness:   1.4%\n",
      "Neutral  :  75.7%\n",
      "Sadness  :   9.3%\n",
      "Surprise :   2.4%\n",
      "======================================\n",
      "=== Emotion probabilities (Face 0) ===\n",
      "Anger    :   3.7%\n",
      "Contempt :   3.6%\n",
      "Disgust  :   0.0%\n",
      "Fear     :   0.2%\n",
      "Happiness:   0.5%\n",
      "Neutral  :  69.8%\n",
      "Sadness  :  20.5%\n",
      "Surprise :   1.7%\n",
      "======================================\n",
      "=== Emotion probabilities (Face 0) ===\n",
      "Anger    :   4.7%\n",
      "Contempt :   2.9%\n",
      "Disgust  :   0.0%\n",
      "Fear     :   0.2%\n",
      "Happiness:   0.4%\n",
      "Neutral  :  62.1%\n",
      "Sadness  :  28.1%\n",
      "Surprise :   1.5%\n",
      "======================================\n",
      "=== Emotion probabilities (Face 0) ===\n",
      "Anger    :   2.0%\n",
      "Contempt :   3.1%\n",
      "Disgust  :   0.0%\n",
      "Fear     :   0.1%\n",
      "Happiness:   0.4%\n",
      "Neutral  :  72.4%\n",
      "Sadness  :  20.5%\n",
      "Surprise :   1.5%\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "from emotiefflib.facial_analysis import EmotiEffLibRecognizer, get_model_list\n",
    "\n",
    "# (옵션) 이미 mediapipe 설치했으면 포즈도 같이 사용 가능\n",
    "try:\n",
    "    import mediapipe as mp\n",
    "    USE_POSE = True\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_pose = mp.solutions.pose\n",
    "except ImportError:\n",
    "    USE_POSE = False\n",
    "\n",
    "# -------------------------------\n",
    "# 1. 장치 / 모델 초기화\n",
    "# -------------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"[INFO] Using device:\", device)\n",
    "print(\"[INFO] Available models:\", get_model_list())\n",
    "\n",
    "# 얼굴 검출기 (MTCNN)\n",
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "\n",
    "# 감정 인식 모델\n",
    "model_name = \"enet_b0_8_best_vgaf\"\n",
    "emotion_recognizer = EmotiEffLibRecognizer(\n",
    "    engine=\"torch\",\n",
    "    model_name=model_name,\n",
    "    device=device,\n",
    ")\n",
    "print(\"[INFO] EmotiEffLib model loaded:\", model_name)\n",
    "print(\"[INFO] Press 'q' to quit\")\n",
    "\n",
    "# AffectNet 8-class 레이블 (문서 기준 순서)\n",
    "EMOTION_LABELS = [\n",
    "    \"Anger\",\n",
    "    \"Contempt\",\n",
    "    \"Disgust\",\n",
    "    \"Fear\",\n",
    "    \"Happiness\",\n",
    "    \"Neutral\",\n",
    "    \"Sadness\",\n",
    "    \"Surprise\",\n",
    "]\n",
    "\n",
    "# FPS 계산용\n",
    "prev_time = time.time()\n",
    "fps = 0.0\n",
    "\n",
    "# 콘솔에 전체 분포 출력 주기 (초)\n",
    "last_print_time = 0.0\n",
    "PRINT_INTERVAL = 1.0  # 1초마다 한번씩만 출력\n",
    "\n",
    "# Pose 초기화\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5,\n",
    "                    min_tracking_confidence=0.5) if USE_POSE else None\n",
    "\n",
    "# -------------------------------\n",
    "# 2. 웹캠 루프\n",
    "# -------------------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"[ERROR] Cannot open webcam\")\n",
    "    raise SystemExit\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"[ERROR] Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        # FPS 계산\n",
    "        now_time = time.time()\n",
    "        dt = now_time - prev_time\n",
    "        if dt > 0:\n",
    "            fps = 1.0 / dt\n",
    "        prev_time = now_time\n",
    "\n",
    "        # BGR -> RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # ---------------- 얼굴 검출 ----------------\n",
    "        boxes, _ = mtcnn.detect(frame_rgb)\n",
    "        faces = mtcnn(frame_rgb)  # (N, 3, H, W) 텐서 or None\n",
    "\n",
    "        all_scores = None  # 모든 얼굴의 확률 저장용\n",
    "\n",
    "        if faces is not None:\n",
    "            facial_images_np = []\n",
    "            for face in faces:\n",
    "                # (C, H, W) -> (H, W, C)\n",
    "                face_np = face.permute(1, 2, 0).cpu().numpy()\n",
    "                # [-1,1] -> [0,255] uint8\n",
    "                face_np = (face_np * 128 + 127.5).clip(0, 255).astype(np.uint8)\n",
    "                facial_images_np.append(face_np)\n",
    "\n",
    "            # 감정 예측: logits=False 로 바로 확률(probabilities) 받기\n",
    "            # emotions: 예측된 감정 레이블 리스트 (문자열)\n",
    "            # scores: shape (N_faces, 8) 의 확률 (0~1)\n",
    "            emotions, scores = emotion_recognizer.predict_emotions(\n",
    "                facial_images_np, logits=False\n",
    "            )\n",
    "            all_scores = np.array(scores)  # (N, 8)\n",
    "\n",
    "            # 콘솔에 1초에 한 번, 첫 번째 얼굴의 전체 분포 출력\n",
    "            if len(all_scores) > 0 and (now_time - last_print_time) >= PRINT_INTERVAL:\n",
    "                print(\"=== Emotion probabilities (Face 0) ===\")\n",
    "                for emo_idx, emo_name in enumerate(EMOTION_LABELS):\n",
    "                    prob = all_scores[0, emo_idx] * 100.0\n",
    "                    print(f\"{emo_name:9s}: {prob:5.1f}%\")\n",
    "                print(\"======================================\")\n",
    "                last_print_time = now_time\n",
    "\n",
    "            # 박스 + 감정 + 확률 오버레이\n",
    "            if boxes is not None:\n",
    "                for i, box in enumerate(boxes):\n",
    "                    if box is None:\n",
    "                        continue\n",
    "                    x1, y1, x2, y2 = [int(b) for b in box]\n",
    "                    emotion = emotions[i]\n",
    "\n",
    "                    # 이 얼굴의 최고 확률\n",
    "                    if all_scores is not None and i < all_scores.shape[0]:\n",
    "                        top_prob = float(all_scores[i].max()) * 100.0\n",
    "                        text = f\"{emotion} {top_prob:.1f}%\"\n",
    "                    else:\n",
    "                        text = emotion\n",
    "\n",
    "                    # 얼굴 박스\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2),\n",
    "                                  (0, 255, 0), 2)\n",
    "                    # 감정+확률 텍스트\n",
    "                    cv2.putText(frame, text, (x1, max(y1 - 10, 10)),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # ---------------- 포즈 추정 (선택) ----------------\n",
    "        if USE_POSE and pose is not None:\n",
    "            results = pose.process(frame_rgb)\n",
    "            if results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame,\n",
    "                    results.pose_landmarks,\n",
    "                    mp_pose.POSE_CONNECTIONS\n",
    "                )\n",
    "\n",
    "        # ---------------- FPS 표시 ----------------\n",
    "        fps_text = f\"FPS: {fps:.1f}\"\n",
    "        cv2.putText(frame, fps_text, (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1.0, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # 화면 출력\n",
    "        cv2.imshow(\"Webcam Emotion + (Pose)\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    cap.release()\n",
    "    if USE_POSE and pose is not None:\n",
    "        pose.close()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36ec7baa-c875-4fdc-99fc-00849631490b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "[INFO] Available EmotiEffLib models: ['enet_b0_8_best_vgaf', 'enet_b0_8_best_afew', 'enet_b2_8', 'enet_b0_8_va_mtl', 'enet_b2_7', 'mbf_va_mtl', 'mobilevit_va_mtl']\n",
      "[INFO] EmotiEffLib model loaded: enet_b0_8_best_vgaf\n",
      "[INFO] MediaPipe Pose enabled\n",
      "[INFO] Press SPACE to record 3s log, 'q' to quit.\n",
      "[INFO] Recording started for 3 seconds...\n",
      "[INFO] Recording finished. Frames: 16\n",
      "[INFO] Saved log: logs\\emotion_log_2025-11-28_16-33-35.csv\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "from emotiefflib.facial_analysis import EmotiEffLibRecognizer, get_model_list\n",
    "\n",
    "# (옵션) mediapipe가 설치되어 있으면 포즈도 같이 사용\n",
    "try:\n",
    "    import mediapipe as mp\n",
    "    USE_POSE = True\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_pose = mp.solutions.pose\n",
    "except ImportError:\n",
    "    USE_POSE = False\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1. 기본 설정 및 모델 초기화\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"[INFO] Using device:\", device)\n",
    "print(\"[INFO] Available EmotiEffLib models:\", get_model_list())\n",
    "\n",
    "# 얼굴 검출기\n",
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "\n",
    "# 감정 인식 모델\n",
    "model_name = \"enet_b0_8_best_vgaf\"\n",
    "emotion_recognizer = EmotiEffLibRecognizer(\n",
    "    engine=\"torch\",\n",
    "    model_name=model_name,\n",
    "    device=device,\n",
    ")\n",
    "print(\"[INFO] EmotiEffLib model loaded:\", model_name)\n",
    "\n",
    "# AffectNet 8-class 레이블 순서 (문서 기준)\n",
    "EMOTION_LABELS = [\n",
    "    \"Anger\",\n",
    "    \"Contempt\",\n",
    "    \"Disgust\",\n",
    "    \"Fear\",\n",
    "    \"Happiness\",\n",
    "    \"Neutral\",\n",
    "    \"Sadness\",\n",
    "    \"Surprise\",\n",
    "]\n",
    "\n",
    "# 로그 저장 폴더\n",
    "LOG_DIR = \"logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# FPS 계산용\n",
    "prev_time = time.time()\n",
    "fps = 0.0\n",
    "\n",
    "# 녹화(로깅) 상태 관리\n",
    "recording = False\n",
    "record_start_time = 0.0\n",
    "record_buffer = []  # 각 프레임의 feature를 리스트로 쌓음\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Pose 초기화(선택)\n",
    "# -----------------------------\n",
    "pose = None\n",
    "if USE_POSE:\n",
    "    mp_pose_module = mp.solutions.pose\n",
    "    pose = mp_pose_module.Pose(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=1,\n",
    "        enable_segmentation=False,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5,\n",
    "    )\n",
    "    print(\"[INFO] MediaPipe Pose enabled\")\n",
    "else:\n",
    "    print(\"[INFO] MediaPipe Pose NOT used (mediapipe not installed)\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3. CSV 저장 함수\n",
    "# -----------------------------\n",
    "def save_log_to_csv(records):\n",
    "    \"\"\"records: list of dict, 각 dict는 한 프레임의 정보\"\"\"\n",
    "    if not records:\n",
    "        print(\"[INFO] No records to save.\")\n",
    "        return\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    filename = os.path.join(LOG_DIR, f\"emotion_log_{ts}.csv\")\n",
    "\n",
    "    # 필드 이름(컬럼)\n",
    "    fieldnames = [\n",
    "        \"t\",\n",
    "        \"fps\",\n",
    "        \"top_emotion\",\n",
    "        \"top_prob\",\n",
    "    ] + [f\"prob_{emo}\" for emo in EMOTION_LABELS]\n",
    "\n",
    "    import csv\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for rec in records:\n",
    "            writer.writerow(rec)\n",
    "\n",
    "    print(f\"[INFO] Saved log: {filename}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4. 메인 루프\n",
    "# -----------------------------\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "if not cap.isOpened():\n",
    "    print(\"[ERROR] Cannot open webcam\")\n",
    "    raise SystemExit\n",
    "\n",
    "print(\"[INFO] Press SPACE to record 3s log, 'q' to quit.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"[ERROR] Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        # FPS 계산\n",
    "        now_time = time.time()\n",
    "        dt = now_time - prev_time\n",
    "        if dt > 0:\n",
    "            fps = 1.0 / dt\n",
    "        prev_time = now_time\n",
    "\n",
    "        # 원본 복사 (표시 용도)\n",
    "        display_frame = frame.copy()\n",
    "\n",
    "        # BGR -> RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # -----------------------------\n",
    "        # A. Pose 추정 (선택)\n",
    "        # -----------------------------\n",
    "        if USE_POSE and pose is not None:\n",
    "            results = pose.process(frame_rgb)\n",
    "            if results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    display_frame,\n",
    "                    results.pose_landmarks,\n",
    "                    mp_pose_module.POSE_CONNECTIONS,\n",
    "                )\n",
    "                # 나중에 feature로 쓰고 싶으면 여기서 x,y,z,visibility 추출해서 기록 가능\n",
    "\n",
    "        # -----------------------------\n",
    "        # B. 얼굴 감정 인식\n",
    "        # -----------------------------\n",
    "        boxes, _ = mtcnn.detect(frame_rgb)\n",
    "        faces = mtcnn(frame_rgb)\n",
    "\n",
    "        # 기본값 (얼굴 없을 때)\n",
    "        top_emotion = \"none\"\n",
    "        top_prob = 0.0\n",
    "        probs_vec = np.zeros(len(EMOTION_LABELS), dtype=float)\n",
    "\n",
    "        if faces is not None:\n",
    "            facial_images_np = []\n",
    "            # faces: (N, 3, H, W) 텐서일 가능성이 큼\n",
    "            if isinstance(faces, torch.Tensor):\n",
    "                if faces.ndim == 3:  # (3,H,W) -> 1개\n",
    "                    faces = faces.unsqueeze(0)\n",
    "                faces_iter = [f for f in faces]\n",
    "            else:\n",
    "                faces_iter = list(faces)\n",
    "\n",
    "            for face in faces_iter:\n",
    "                face_np = face.permute(1, 2, 0).cpu().numpy()\n",
    "                face_np = (face_np * 128 + 127.5).clip(0, 255).astype(np.uint8)\n",
    "                facial_images_np.append(face_np)\n",
    "\n",
    "            if len(facial_images_np) > 0:\n",
    "                emotions, scores = emotion_recognizer.predict_emotions(\n",
    "                    facial_images_np, logits=False  # 확률로 받기\n",
    "                )\n",
    "                scores = np.array(scores)  # (N, 8)\n",
    "\n",
    "                # 첫 번째 얼굴 기준으로 top emotion / prob 계산\n",
    "                probs_vec = scores[0]  # shape: (8,)\n",
    "                top_idx = int(np.argmax(probs_vec))\n",
    "                top_emotion = EMOTION_LABELS[top_idx]\n",
    "                top_prob = float(probs_vec[top_idx]) * 100.0\n",
    "\n",
    "                # 화면에 박스 + 감정 표시 (모든 얼굴에 대해)\n",
    "                if boxes is not None:\n",
    "                    for i, box in enumerate(boxes):\n",
    "                        if box is None:\n",
    "                            continue\n",
    "                        x1, y1, x2, y2 = [int(b) for b in box]\n",
    "                        emo = emotions[i]\n",
    "                        emo_prob = float(scores[i].max()) * 100.0\n",
    "\n",
    "                        text = f\"{emo} {emo_prob:.1f}%\"\n",
    "                        cv2.rectangle(display_frame, (x1, y1), (x2, y2),\n",
    "                                      (0, 255, 0), 2)\n",
    "                        cv2.putText(display_frame, text,\n",
    "                                    (x1, max(y1 - 10, 10)),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                    0.7, (0, 255, 0), 2,\n",
    "                                    cv2.LINE_AA)\n",
    "        else:\n",
    "            cv2.putText(display_frame, \"No face detected\",\n",
    "                        (20, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        1.0, (0, 0, 255), 2,\n",
    "                        cv2.LINE_AA)\n",
    "\n",
    "        # -----------------------------\n",
    "        # C. 녹화(로깅) 상태 처리\n",
    "        # -----------------------------\n",
    "        if recording:\n",
    "            t_rel = now_time - record_start_time  # 녹화 시작으로부터 경과 시간\n",
    "            # 로그용 dict 하나 만들기\n",
    "            record = {\n",
    "                \"t\": t_rel,\n",
    "                \"fps\": fps,\n",
    "                \"top_emotion\": top_emotion,\n",
    "                \"top_prob\": top_prob,\n",
    "            }\n",
    "            for idx, emo_name in enumerate(EMOTION_LABELS):\n",
    "                record[f\"prob_{emo_name}\"] = float(probs_vec[idx]) * 100.0\n",
    "            record_buffer.append(record)\n",
    "\n",
    "            # 화면에 REC 표시\n",
    "            cv2.putText(display_frame, \"REC\",\n",
    "                        (10, 70),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        1.0, (0, 0, 255), 2,\n",
    "                        cv2.LINE_AA)\n",
    "\n",
    "            # 3초 지나면 자동 종료 + 저장\n",
    "            if t_rel >= 3.0:\n",
    "                recording = False\n",
    "                print(f\"[INFO] Recording finished. Frames: {len(record_buffer)}\")\n",
    "                save_log_to_csv(record_buffer)\n",
    "                record_buffer = []  # 비우기\n",
    "\n",
    "        # -----------------------------\n",
    "        # D. FPS 표시\n",
    "        # -----------------------------\n",
    "        fps_text = f\"FPS: {fps:.1f}\"\n",
    "        cv2.putText(display_frame, fps_text, (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1.0, (0, 255, 255), 2,\n",
    "                    cv2.LINE_AA)\n",
    "\n",
    "        # 화면 출력\n",
    "        cv2.imshow(\"Emotion + (Pose) + Logging\", display_frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "        elif key == ord(\" \"):  # SPACE 누르면 3초 녹화 시작\n",
    "            if not recording:\n",
    "                recording = True\n",
    "                record_start_time = now_time\n",
    "                record_buffer = []\n",
    "                print(\"[INFO] Recording started for 3 seconds...\")\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    if USE_POSE and pose is not None:\n",
    "        pose.close()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1475afd4-802c-469a-9d38-51f8583cd77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\OSS\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "[INFO] Available EmotiEffLib models: ['enet_b0_8_best_vgaf', 'enet_b0_8_best_afew', 'enet_b2_8', 'enet_b0_8_va_mtl', 'enet_b2_7', 'mbf_va_mtl', 'mobilevit_va_mtl']\n",
      "[INFO] EmotiEffLib model loaded: enet_b0_8_best_vgaf\n",
      "[INFO] MediaPipe Pose enabled\n",
      "[INFO] Press SPACE to record 3s log, 'q' to quit.\n",
      "[INFO] Recording started for 3 seconds...\n",
      "[INFO] Recording finished. Frames: 17\n",
      "[INFO] Saved log: logs\\emotion_log_2025-11-28_17-07-54.csv\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "from emotiefflib.facial_analysis import EmotiEffLibRecognizer, get_model_list\n",
    "\n",
    "# (옵션) mediapipe가 설치되어 있으면 포즈도 같이 사용\n",
    "try:\n",
    "    import mediapipe as mp\n",
    "    USE_POSE = True\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_pose = mp.solutions.pose\n",
    "except ImportError:\n",
    "    USE_POSE = False\n",
    "\n",
    "# -----------------------------\n",
    "# 0. Pose 랜드마크 이름 (33개)\n",
    "# -----------------------------\n",
    "LANDMARK_NAMES = [\n",
    "    \"nose\",               # 0\n",
    "    \"left_eye_inner\",     # 1\n",
    "    \"left_eye\",           # 2\n",
    "    \"left_eye_outer\",     # 3\n",
    "    \"right_eye_inner\",    # 4\n",
    "    \"right_eye\",          # 5\n",
    "    \"right_eye_outer\",    # 6\n",
    "    \"left_ear\",           # 7\n",
    "    \"right_ear\",          # 8\n",
    "    \"mouth_left\",         # 9\n",
    "    \"mouth_right\",        # 10\n",
    "    \"left_shoulder\",      # 11\n",
    "    \"right_shoulder\",     # 12\n",
    "    \"left_elbow\",         # 13\n",
    "    \"right_elbow\",        # 14\n",
    "    \"left_wrist\",         # 15\n",
    "    \"right_wrist\",        # 16\n",
    "    \"left_pinky\",         # 17\n",
    "    \"right_pinky\",        # 18\n",
    "    \"left_index\",         # 19\n",
    "    \"right_index\",        # 20\n",
    "    \"left_thumb\",         # 21\n",
    "    \"right_thumb\",        # 22\n",
    "    \"left_hip\",           # 23\n",
    "    \"right_hip\",          # 24\n",
    "    \"left_knee\",          # 25\n",
    "    \"right_knee\",         # 26\n",
    "    \"left_ankle\",         # 27\n",
    "    \"right_ankle\",        # 28\n",
    "    \"left_heel\",          # 29\n",
    "    \"right_heel\",         # 30\n",
    "    \"left_foot_index\",    # 31\n",
    "    \"right_foot_index\",   # 32\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 1. 기본 설정 및 모델 초기화\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"[INFO] Using device:\", device)\n",
    "print(\"[INFO] Available EmotiEffLib models:\", get_model_list())\n",
    "\n",
    "# 얼굴 검출기\n",
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "\n",
    "# 감정 인식 모델\n",
    "model_name = \"enet_b0_8_best_vgaf\"\n",
    "emotion_recognizer = EmotiEffLibRecognizer(\n",
    "    engine=\"torch\",\n",
    "    model_name=model_name,\n",
    "    device=device,\n",
    ")\n",
    "print(\"[INFO] EmotiEffLib model loaded:\", model_name)\n",
    "\n",
    "# AffectNet 8-class 레이블 순서 (문서 기준)\n",
    "EMOTION_LABELS = [\n",
    "    \"Anger\",\n",
    "    \"Contempt\",\n",
    "    \"Disgust\",\n",
    "    \"Fear\",\n",
    "    \"Happiness\",\n",
    "    \"Neutral\",\n",
    "    \"Sadness\",\n",
    "    \"Surprise\",\n",
    "]\n",
    "\n",
    "# 로그 저장 폴더\n",
    "LOG_DIR = \"logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# FPS 계산용\n",
    "prev_time = time.time()\n",
    "fps = 0.0\n",
    "\n",
    "# 녹화(로깅) 상태 관리\n",
    "recording = False\n",
    "record_start_time = 0.0\n",
    "record_buffer = []  # 각 프레임의 feature를 리스트로 쌓음\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Pose 초기화(선택)\n",
    "# -----------------------------\n",
    "pose = None\n",
    "if USE_POSE:\n",
    "    mp_pose_module = mp.solutions.pose\n",
    "    pose = mp_pose_module.Pose(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=1,\n",
    "        enable_segmentation=False,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5,\n",
    "    )\n",
    "    print(\"[INFO] MediaPipe Pose enabled\")\n",
    "else:\n",
    "    print(\"[INFO] MediaPipe Pose NOT used (mediapipe not installed)\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. CSV 저장 함수\n",
    "# -----------------------------\n",
    "def save_log_to_csv(records):\n",
    "    \"\"\"records: list of dict, 각 dict는 한 프레임의 정보\"\"\"\n",
    "    if not records:\n",
    "        print(\"[INFO] No records to save.\")\n",
    "        return\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    filename = os.path.join(LOG_DIR, f\"emotion_log_{ts}.csv\")\n",
    "\n",
    "    # 필드 이름(컬럼)\n",
    "    fieldnames = [\n",
    "        \"t\",\n",
    "        \"fps\",\n",
    "        \"top_emotion\",\n",
    "        \"top_prob\",\n",
    "    ] + [f\"prob_{emo}\" for emo in EMOTION_LABELS]\n",
    "\n",
    "    # Pose 랜드마크 컬럼 추가 (각 랜드마크당 x,y,z,vis)\n",
    "    for name in LANDMARK_NAMES:\n",
    "        fieldnames.extend([\n",
    "            f\"{name}_x\",\n",
    "            f\"{name}_y\",\n",
    "            f\"{name}_z\",\n",
    "            f\"{name}_vis\",\n",
    "        ])\n",
    "\n",
    "    import csv\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for rec in records:\n",
    "            writer.writerow(rec)\n",
    "\n",
    "    print(f\"[INFO] Saved log: {filename}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. 메인 루프\n",
    "# -----------------------------\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "if not cap.isOpened():\n",
    "    print(\"[ERROR] Cannot open webcam\")\n",
    "    raise SystemExit\n",
    "\n",
    "print(\"[INFO] Press SPACE to record 3s log, 'q' to quit.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"[ERROR] Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        # FPS 계산\n",
    "        now_time = time.time()\n",
    "        dt = now_time - prev_time\n",
    "        if dt > 0:\n",
    "            fps = 1.0 / dt\n",
    "        prev_time = now_time\n",
    "\n",
    "        # 원본 복사 (표시 용도)\n",
    "        display_frame = frame.copy()\n",
    "\n",
    "        # BGR -> RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # -----------------------------\n",
    "        # A. Pose 추정 (선택) + 랜드마크 저장 준비\n",
    "        # -----------------------------\n",
    "        pose_landmarks = None  # 이 프레임에서의 랜드마크 리스트 (없으면 None)\n",
    "\n",
    "        if USE_POSE and pose is not None:\n",
    "            results = pose.process(frame_rgb)\n",
    "            if results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    display_frame,\n",
    "                    results.pose_landmarks,\n",
    "                    mp_pose_module.POSE_CONNECTIONS,\n",
    "                )\n",
    "                pose_landmarks = results.pose_landmarks.landmark  # 길이 33 리스트\n",
    "\n",
    "        # -----------------------------\n",
    "        # B. 얼굴 감정 인식\n",
    "        # -----------------------------\n",
    "        boxes, _ = mtcnn.detect(frame_rgb)\n",
    "        faces = mtcnn(frame_rgb)\n",
    "\n",
    "        # 기본값 (얼굴 없을 때)\n",
    "        top_emotion = \"none\"\n",
    "        top_prob = 0.0\n",
    "        probs_vec = np.zeros(len(EMOTION_LABELS), dtype=float)\n",
    "\n",
    "        if faces is not None:\n",
    "            facial_images_np = []\n",
    "            # faces: (N, 3, H, W) 텐서일 가능성이 큼\n",
    "            if isinstance(faces, torch.Tensor):\n",
    "                if faces.ndim == 3:  # (3,H,W) -> 1개\n",
    "                    faces = faces.unsqueeze(0)\n",
    "                faces_iter = [f for f in faces]\n",
    "            else:\n",
    "                faces_iter = list(faces)\n",
    "\n",
    "            for face in faces_iter:\n",
    "                face_np = face.permute(1, 2, 0).cpu().numpy()\n",
    "                face_np = (face_np * 128 + 127.5).clip(0, 255).astype(np.uint8)\n",
    "                facial_images_np.append(face_np)\n",
    "\n",
    "            if len(facial_images_np) > 0:\n",
    "                emotions, scores = emotion_recognizer.predict_emotions(\n",
    "                    facial_images_np, logits=False  # 확률로 받기\n",
    "                )\n",
    "                scores = np.array(scores)  # (N, 8)\n",
    "\n",
    "                # 첫 번째 얼굴 기준으로 top emotion / prob 계산\n",
    "                probs_vec = scores[0]  # shape: (8,)\n",
    "                top_idx = int(np.argmax(probs_vec))\n",
    "                top_emotion = EMOTION_LABELS[top_idx]\n",
    "                top_prob = float(probs_vec[top_idx]) * 100.0\n",
    "\n",
    "                # 화면에 박스 + 감정 표시 (모든 얼굴에 대해)\n",
    "                if boxes is not None:\n",
    "                    for i, box in enumerate(boxes):\n",
    "                        if box is None:\n",
    "                            continue\n",
    "                        x1, y1, x2, y2 = [int(b) for b in box]\n",
    "                        emo = emotions[i]\n",
    "                        emo_prob = float(scores[i].max()) * 100.0\n",
    "\n",
    "                        text = f\"{emo} {emo_prob:.1f}%\"\n",
    "                        cv2.rectangle(display_frame, (x1, y1), (x2, y2),\n",
    "                                      (0, 255, 0), 2)\n",
    "                        cv2.putText(display_frame, text,\n",
    "                                    (x1, max(y1 - 10, 10)),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                    0.7, (0, 255, 0), 2,\n",
    "                                    cv2.LINE_AA)\n",
    "        else:\n",
    "            cv2.putText(display_frame, \"No face detected\",\n",
    "                        (20, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        1.0, (0, 0, 255), 2,\n",
    "                        cv2.LINE_AA)\n",
    "\n",
    "        # -----------------------------\n",
    "        # C. 녹화(로깅) 상태 처리\n",
    "        # -----------------------------\n",
    "        if recording:\n",
    "            t_rel = now_time - record_start_time  # 녹화 시작으로부터 경과 시간\n",
    "\n",
    "            # 기본 로그 필드\n",
    "            record = {\n",
    "                \"t\": t_rel,\n",
    "                \"fps\": fps,\n",
    "                \"top_emotion\": top_emotion,\n",
    "                \"top_prob\": top_prob,\n",
    "            }\n",
    "            for idx, emo_name in enumerate(EMOTION_LABELS):\n",
    "                record[f\"prob_{emo_name}\"] = float(probs_vec[idx]) * 100.0\n",
    "\n",
    "            # Pose 랜드마크 값 기본은 -999로 채움\n",
    "            for name in LANDMARK_NAMES:\n",
    "                record[f\"{name}_x\"] = -999.0\n",
    "                record[f\"{name}_y\"] = -999.0\n",
    "                record[f\"{name}_z\"] = -999.0\n",
    "                record[f\"{name}_vis\"] = -999.0\n",
    "\n",
    "            # 이 프레임에서 포즈가 보였으면, visibility > 0.5 인 것만 값 채우기\n",
    "            if pose_landmarks is not None:\n",
    "                for idx, name in enumerate(LANDMARK_NAMES):\n",
    "                    if idx >= len(pose_landmarks):\n",
    "                        break\n",
    "                    lm = pose_landmarks[idx]\n",
    "                    vis = float(lm.visibility) if lm.visibility is not None else 0.0\n",
    "                    if vis > 0.5:  # \"보이는\" 기준, 필요하면 조정 가능\n",
    "                        record[f\"{name}_x\"] = float(lm.x)\n",
    "                        record[f\"{name}_y\"] = float(lm.y)\n",
    "                        record[f\"{name}_z\"] = float(lm.z)\n",
    "                        record[f\"{name}_vis\"] = vis\n",
    "                    # vis <= 0.5 인 경우는 기본값 -999 유지\n",
    "\n",
    "            record_buffer.append(record)\n",
    "\n",
    "            # 화면에 REC 표시\n",
    "            cv2.putText(display_frame, \"REC\",\n",
    "                        (10, 70),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        1.0, (0, 0, 255), 2,\n",
    "                        cv2.LINE_AA)\n",
    "\n",
    "            # 3초 지나면 자동 종료 + 저장\n",
    "            if t_rel >= 3.0:\n",
    "                recording = False\n",
    "                print(f\"[INFO] Recording finished. Frames: {len(record_buffer)}\")\n",
    "                save_log_to_csv(record_buffer)\n",
    "                record_buffer = []  # 비우기\n",
    "\n",
    "        # -----------------------------\n",
    "        # D. FPS 표시\n",
    "        # -----------------------------\n",
    "        fps_text = f\"FPS: {fps:.1f}\"\n",
    "        cv2.putText(display_frame, fps_text, (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1.0, (0, 255, 255), 2,\n",
    "                    cv2.LINE_AA)\n",
    "\n",
    "        # 화면 출력\n",
    "        cv2.imshow(\"Emotion + (Pose) + Logging\", display_frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "        elif key == ord(\" \"):  # SPACE 누르면 3초 녹화 시작\n",
    "            if not recording:\n",
    "                recording = True\n",
    "                record_start_time = now_time\n",
    "                record_buffer = []\n",
    "                print(\"[INFO] Recording started for 3 seconds...\")\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    if USE_POSE and pose is not None:\n",
    "        pose.close()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d97411c-c508-4888-9089-7d7e9cb939fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 1 logs.\n",
      "[DONE] emotion_log_2025-11-28_17-07-54_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21236\\867357523.py:20: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  nose_x = df_vis['nose_x'].rolling(window=5).mean().fillna(method='bfill')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21236\\867357523.py:21: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  nose_y = df_vis['nose_y'].rolling(window=5).mean().fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# 설정\n",
    "LOG_DIR = \"logs\"\n",
    "OUTPUT_DIR = \"preprocessed\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def detect_head_gesture(df_vis):\n",
    "    \"\"\"\n",
    "    코(nose)의 움직임을 통해 끄덕임(Nodding)이나 가로저음(Shaking)을 감지\n",
    "    \"\"\"\n",
    "    if df_vis['nose_vis'].mean() < 0.5:\n",
    "        return \"Not Detected\"\n",
    "\n",
    "    # 이동 평균으로 노이즈 제거\n",
    "    nose_x = df_vis['nose_x'].rolling(window=5).mean().fillna(method='bfill')\n",
    "    nose_y = df_vis['nose_y'].rolling(window=5).mean().fillna(method='bfill')\n",
    "\n",
    "    # 움직임의 분산(Variance) 계산 (움직임의 크기)\n",
    "    var_x = nose_x.var() * 10000  # 스케일 보정\n",
    "    var_y = nose_y.var() * 10000\n",
    "\n",
    "    # 움직임이 너무 적으면 Static\n",
    "    if var_x < 0.05 and var_y < 0.05:\n",
    "        return \"Static (Still)\"\n",
    "\n",
    "    # X축 움직임이 Y축보다 현저히 크면 -> Shaking (No/Confusion)\n",
    "    if var_x > var_y * 1.5:\n",
    "        return \"Head Shaking (Negative/Confusion)\"\n",
    "    \n",
    "    # Y축 움직임이 X축보다 현저히 크면 -> Nodding (Yes/Agreed)\n",
    "    if var_y > var_x * 1.5:\n",
    "        return \"Head Nodding (Positive/Understood)\"\n",
    "\n",
    "    return \"Dynamic (Moving)\"\n",
    "\n",
    "def analyze_posture_lean(df_vis):\n",
    "    \"\"\"\n",
    "    어깨의 Z축 변화를 통해 몸을 앞으로 기울였는지(집중) 뒤로 뺐는지(이완) 판단\n",
    "    MediaPipe에서 Z값은 카메라에 가까울수록 작아짐 (음수 방향)\n",
    "    \"\"\"\n",
    "    # 어깨가 보이는 프레임만\n",
    "    if df_vis['left_shoulder_vis'].mean() < 0.5:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    # 초반 30% vs 후반 30% Z값 비교\n",
    "    n = len(df_vis)\n",
    "    start_z = df_vis[['left_shoulder_z', 'right_shoulder_z']].iloc[:int(n*0.3)].mean().mean()\n",
    "    end_z = df_vis[['left_shoulder_z', 'right_shoulder_z']].iloc[int(n*0.7):].mean().mean()\n",
    "    \n",
    "    diff = start_z - end_z # 양수면 나중이 더 작아짐(가까워짐)\n",
    "\n",
    "    # 임계값 (실험적으로 조정 필요)\n",
    "    if diff > 0.05: \n",
    "        return \"Leaning Forward (High Engagement)\"\n",
    "    elif diff < -0.05:\n",
    "        return \"Leaning Backward (Relaxed/Bored)\"\n",
    "    else:\n",
    "        return \"Stable Posture\"\n",
    "\n",
    "def analyze_behavior(file_path):\n",
    "    try:\n",
    "        # -999를 NaN으로 변환하여 로드\n",
    "        df = pd.read_csv(file_path)\n",
    "        df.replace(-999.0, np.nan, inplace=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERR] Load failed: {file_path}\")\n",
    "        return None\n",
    "\n",
    "    if len(df) < 10: return None\n",
    "\n",
    "    # 1. Pose 데이터가 유효한 행만 추출 (코가 보이는 프레임)\n",
    "    df_vis = df.dropna(subset=['nose_x'])\n",
    "    has_pose = len(df_vis) > len(df) * 0.5  # 전체의 50% 이상 포즈가 잡혔는지\n",
    "\n",
    "    # 2. 감정 분석 (Baseline 제거 로직)\n",
    "    # Neutral, Sadness는 '기본 상태'로 간주. 그 외 감정의 스파이크 감지\n",
    "    emotion_cols = [c for c in df.columns if c.startswith(\"prob_\")]\n",
    "    avg_emotions = df[emotion_cols].mean()\n",
    "    \n",
    "    # 주요 감정 추출 (Neutral 제외 Top 1)\n",
    "    sorted_emotions = avg_emotions.drop(\"prob_Neutral\", errors='ignore').sort_values(ascending=False)\n",
    "    dominant_sub_emotion = sorted_emotions.index[0].replace(\"prob_\", \"\")\n",
    "    dominant_score = sorted_emotions.iloc[0]\n",
    "\n",
    "    # 감정 해석\n",
    "    emotion_summary = f\"Mainly Neutral, but shows traces of {dominant_sub_emotion} ({dominant_score:.1f}%)\"\n",
    "    if dominant_sub_emotion == \"Sadness\":\n",
    "        emotion_context = \"Concentrated / Serious\"\n",
    "    elif dominant_sub_emotion in [\"Anger\", \"Disgust\", \"Contempt\"]:\n",
    "        emotion_context = \"Dissatisfied / Uncomfortable\"\n",
    "    elif dominant_sub_emotion in [\"Fear\", \"Surprise\"]:\n",
    "        emotion_context = \"Confused / Surprised\"\n",
    "    elif dominant_sub_emotion == \"Happiness\":\n",
    "        emotion_context = \"Satisfied / Amused\"\n",
    "    else:\n",
    "        emotion_context = \"Passive\"\n",
    "\n",
    "    # 3. 행동(Pose) 분석\n",
    "    gesture = \"Not Detected\"\n",
    "    posture = \"Unknown\"\n",
    "    \n",
    "    if has_pose:\n",
    "        gesture = detect_head_gesture(df_vis)\n",
    "        posture = analyze_posture_lean(df_vis)\n",
    "\n",
    "    # 4. LLM용 최종 요약 생성 (JSON)\n",
    "    result = {\n",
    "        \"metadata\": {\n",
    "            \"duration\": f\"{df['t'].max():.1f}s\",\n",
    "            \"fps\": f\"{df['fps'].mean():.1f}\"\n",
    "        },\n",
    "        \"behavior_analysis\": {\n",
    "            \"facial_expression\": emotion_context,\n",
    "            \"detailed_emotion\": emotion_summary,\n",
    "            \"head_gesture\": gesture,\n",
    "            \"body_posture\": posture\n",
    "        },\n",
    "        \"final_interpretation_for_llm\": \"\"\n",
    "    }\n",
    "\n",
    "    # 5. 종합 해석 문자열 생성 (Rule-based)\n",
    "    # 이 부분이 LLM as Judge의 핵심 Input이 됩니다.\n",
    "    interpretation = []\n",
    "    \n",
    "    if \"Forward\" in posture:\n",
    "        interpretation.append(\"User is highly engaged and leaning in to read details.\")\n",
    "    elif \"Backward\" in posture:\n",
    "        interpretation.append(\"User is sitting back, possibly bored or just skimming.\")\n",
    "        \n",
    "    if \"Nodding\" in gesture:\n",
    "        interpretation.append(\"User is nodding, indicating agreement or understanding.\")\n",
    "    elif \"Shaking\" in gesture:\n",
    "        interpretation.append(\"User is shaking head, indicating confusion or disagreement.\")\n",
    "        \n",
    "    if \"Dissatisfied\" in emotion_context:\n",
    "        interpretation.append(\"Facial expressions show signs of dissatisfaction or frustration.\")\n",
    "    elif \"Confused\" in emotion_context:\n",
    "        interpretation.append(\"User seems confused by the content.\")\n",
    "    \n",
    "    if not interpretation:\n",
    "        interpretation.append(\"User showed no significant non-verbal reaction (Passive reading).\")\n",
    "        \n",
    "    result[\"final_interpretation_for_llm\"] = \" \".join(interpretation)\n",
    "\n",
    "    return result\n",
    "\n",
    "def process_all():\n",
    "    files = glob.glob(os.path.join(LOG_DIR, \"*.csv\"))\n",
    "    print(f\"[INFO] Found {len(files)} logs.\")\n",
    "    \n",
    "    for f in files:\n",
    "        res = analyze_behavior(f)\n",
    "        if res:\n",
    "            out_name = os.path.basename(f).replace(\".csv\", \"_analysis.json\")\n",
    "            with open(os.path.join(OUTPUT_DIR, out_name), \"w\", encoding=\"utf-8\") as json_f:\n",
    "                json.dump(res, json_f, indent=4, ensure_ascii=False)\n",
    "            print(f\"[DONE] {out_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe38e4-8fdc-4343-98b1-f65c03c75354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (OSS)",
   "language": "python",
   "name": "oss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
