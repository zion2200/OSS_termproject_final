{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8416e147-536e-4cbd-8675-51f330eb8804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\OSS\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.9.1+cu126\n",
      "cuda: True / NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "cv2: 4.11.0\n",
      "timm: 0.9.12\n",
      "models: ['enet_b0_8_best_vgaf', 'enet_b0_8_best_afew', 'enet_b2_8', 'enet_b0_8_va_mtl', 'enet_b2_7']\n",
      "âœ… EmotiEffLib + MTCNN ì´ˆê¸°í™” ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import torch, cv2, facenet_pytorch, timm, emotiefflib, numpy as np\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda:\", torch.cuda.is_available(), \"/\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"no gpu\")\n",
    "print(\"cv2:\", cv2.__version__)\n",
    "print(\"timm:\", timm.__version__)\n",
    "\n",
    "from emotiefflib.facial_analysis import EmotiEffLibRecognizer, get_model_list\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "print(\"models:\", get_model_list()[:5])\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "rec = EmotiEffLibRecognizer(engine=\"torch\", model_name=\"enet_b0_8_best_vgaf\", device=device)\n",
    "\n",
    "print(\"âœ… EmotiEffLib + MTCNN ì´ˆê¸°í™” ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1475afd4-802c-469a-9d38-51f8583cd77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "[INFO] Available EmotiEffLib models: ['enet_b0_8_best_vgaf', 'enet_b0_8_best_afew', 'enet_b2_8', 'enet_b0_8_va_mtl', 'enet_b2_7', 'mbf_va_mtl', 'mobilevit_va_mtl']\n",
      "[INFO] EmotiEffLib model loaded: enet_b0_8_best_vgaf\n",
      "[INFO] MediaPipe Pose enabled\n",
      "[INFO] Press SPACE to record 3s log, 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "from emotiefflib.facial_analysis import EmotiEffLibRecognizer, get_model_list\n",
    "\n",
    "# (ì˜µì…˜) mediapipeê°€ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ í¬ì¦ˆë„ ê°™ì´ ì‚¬ìš©\n",
    "try:\n",
    "    import mediapipe as mp\n",
    "    USE_POSE = True\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_pose = mp.solutions.pose\n",
    "except ImportError:\n",
    "    USE_POSE = False\n",
    "\n",
    "# -----------------------------\n",
    "# 0. Pose ëœë“œë§ˆí¬ ì´ë¦„ (33ê°œ)\n",
    "# -----------------------------\n",
    "LANDMARK_NAMES = [\n",
    "    \"nose\",               # 0\n",
    "    \"left_eye_inner\",     # 1\n",
    "    \"left_eye\",           # 2\n",
    "    \"left_eye_outer\",     # 3\n",
    "    \"right_eye_inner\",    # 4\n",
    "    \"right_eye\",          # 5\n",
    "    \"right_eye_outer\",    # 6\n",
    "    \"left_ear\",           # 7\n",
    "    \"right_ear\",          # 8\n",
    "    \"mouth_left\",         # 9\n",
    "    \"mouth_right\",        # 10\n",
    "    \"left_shoulder\",      # 11\n",
    "    \"right_shoulder\",     # 12\n",
    "    \"left_elbow\",         # 13\n",
    "    \"right_elbow\",        # 14\n",
    "    \"left_wrist\",         # 15\n",
    "    \"right_wrist\",        # 16\n",
    "    \"left_pinky\",         # 17\n",
    "    \"right_pinky\",        # 18\n",
    "    \"left_index\",         # 19\n",
    "    \"right_index\",        # 20\n",
    "    \"left_thumb\",         # 21\n",
    "    \"right_thumb\",        # 22\n",
    "    \"left_hip\",           # 23\n",
    "    \"right_hip\",          # 24\n",
    "    \"left_knee\",          # 25\n",
    "    \"right_knee\",         # 26\n",
    "    \"left_ankle\",         # 27\n",
    "    \"right_ankle\",        # 28\n",
    "    \"left_heel\",          # 29\n",
    "    \"right_heel\",         # 30\n",
    "    \"left_foot_index\",    # 31\n",
    "    \"right_foot_index\",   # 32\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 1. ê¸°ë³¸ ì„¤ì • ë° ëª¨ë¸ ì´ˆê¸°í™”\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"[INFO] Using device:\", device)\n",
    "print(\"[INFO] Available EmotiEffLib models:\", get_model_list())\n",
    "\n",
    "# ì–¼êµ´ ê²€ì¶œê¸°\n",
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "\n",
    "# ê°ì • ì¸ì‹ ëª¨ë¸\n",
    "model_name = \"enet_b0_8_best_vgaf\"\n",
    "emotion_recognizer = EmotiEffLibRecognizer(\n",
    "    engine=\"torch\",\n",
    "    model_name=model_name,\n",
    "    device=device,\n",
    ")\n",
    "print(\"[INFO] EmotiEffLib model loaded:\", model_name)\n",
    "\n",
    "# AffectNet 8-class ë ˆì´ë¸” ìˆœì„œ (ë¬¸ì„œ ê¸°ì¤€)\n",
    "EMOTION_LABELS = [\n",
    "    \"Anger\",\n",
    "    \"Contempt\",\n",
    "    \"Disgust\",\n",
    "    \"Fear\",\n",
    "    \"Happiness\",\n",
    "    \"Neutral\",\n",
    "    \"Sadness\",\n",
    "    \"Surprise\",\n",
    "]\n",
    "\n",
    "# ë¡œê·¸ ì €ì¥ í´ë”\n",
    "LOG_DIR = \"logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# FPS ê³„ì‚°ìš©\n",
    "prev_time = time.time()\n",
    "fps = 0.0\n",
    "\n",
    "# ë…¹í™”(ë¡œê¹…) ìƒíƒœ ê´€ë¦¬\n",
    "recording = False\n",
    "record_start_time = 0.0\n",
    "record_buffer = []  # ê° í”„ë ˆì„ì˜ featureë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ìŒ“ìŒ\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Pose ì´ˆê¸°í™”(ì„ íƒ)\n",
    "# -----------------------------\n",
    "pose = None\n",
    "if USE_POSE:\n",
    "    mp_pose_module = mp.solutions.pose\n",
    "    pose = mp_pose_module.Pose(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=1,\n",
    "        enable_segmentation=False,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5,\n",
    "    )\n",
    "    print(\"[INFO] MediaPipe Pose enabled\")\n",
    "else:\n",
    "    print(\"[INFO] MediaPipe Pose NOT used (mediapipe not installed)\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. CSV ì €ì¥ í•¨ìˆ˜\n",
    "# -----------------------------\n",
    "def save_log_to_csv(records):\n",
    "    \"\"\"records: list of dict, ê° dictëŠ” í•œ í”„ë ˆì„ì˜ ì •ë³´\"\"\"\n",
    "    if not records:\n",
    "        print(\"[INFO] No records to save.\")\n",
    "        return\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    filename = os.path.join(LOG_DIR, f\"emotion_log_{ts}.csv\")\n",
    "\n",
    "    # í•„ë“œ ì´ë¦„(ì»¬ëŸ¼)\n",
    "    fieldnames = [\n",
    "        \"t\",\n",
    "        \"fps\",\n",
    "        \"top_emotion\",\n",
    "        \"top_prob\",\n",
    "    ] + [f\"prob_{emo}\" for emo in EMOTION_LABELS]\n",
    "\n",
    "    # Pose ëœë“œë§ˆí¬ ì»¬ëŸ¼ ì¶”ê°€ (ê° ëœë“œë§ˆí¬ë‹¹ x,y,z,vis)\n",
    "    for name in LANDMARK_NAMES:\n",
    "        fieldnames.extend([\n",
    "            f\"{name}_x\",\n",
    "            f\"{name}_y\",\n",
    "            f\"{name}_z\",\n",
    "            f\"{name}_vis\",\n",
    "        ])\n",
    "\n",
    "    import csv\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for rec in records:\n",
    "            writer.writerow(rec)\n",
    "\n",
    "    print(f\"[INFO] Saved log: {filename}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. ë©”ì¸ ë£¨í”„\n",
    "# -----------------------------\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "if not cap.isOpened():\n",
    "    print(\"[ERROR] Cannot open webcam\")\n",
    "    raise SystemExit\n",
    "\n",
    "print(\"[INFO] Press SPACE to record 3s log, 'q' to quit.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"[ERROR] Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        # FPS ê³„ì‚°\n",
    "        now_time = time.time()\n",
    "        dt = now_time - prev_time\n",
    "        if dt > 0:\n",
    "            fps = 1.0 / dt\n",
    "        prev_time = now_time\n",
    "\n",
    "        # ì›ë³¸ ë³µì‚¬ (í‘œì‹œ ìš©ë„)\n",
    "        display_frame = frame.copy()\n",
    "\n",
    "        # BGR -> RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # -----------------------------\n",
    "        # A. Pose ì¶”ì • (ì„ íƒ) + ëœë“œë§ˆí¬ ì €ì¥ ì¤€ë¹„\n",
    "        # -----------------------------\n",
    "        pose_landmarks = None  # ì´ í”„ë ˆì„ì—ì„œì˜ ëœë“œë§ˆí¬ ë¦¬ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ None)\n",
    "\n",
    "        if USE_POSE and pose is not None:\n",
    "            results = pose.process(frame_rgb)\n",
    "            if results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    display_frame,\n",
    "                    results.pose_landmarks,\n",
    "                    mp_pose_module.POSE_CONNECTIONS,\n",
    "                )\n",
    "                pose_landmarks = results.pose_landmarks.landmark  # ê¸¸ì´ 33 ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "        # -----------------------------\n",
    "        # B. ì–¼êµ´ ê°ì • ì¸ì‹\n",
    "        # -----------------------------\n",
    "        boxes, _ = mtcnn.detect(frame_rgb)\n",
    "        faces = mtcnn(frame_rgb)\n",
    "\n",
    "        # ê¸°ë³¸ê°’ (ì–¼êµ´ ì—†ì„ ë•Œ)\n",
    "        top_emotion = \"none\"\n",
    "        top_prob = 0.0\n",
    "        probs_vec = np.zeros(len(EMOTION_LABELS), dtype=float)\n",
    "\n",
    "        if faces is not None:\n",
    "            facial_images_np = []\n",
    "            # faces: (N, 3, H, W) í…ì„œì¼ ê°€ëŠ¥ì„±ì´ í¼\n",
    "            if isinstance(faces, torch.Tensor):\n",
    "                if faces.ndim == 3:  # (3,H,W) -> 1ê°œ\n",
    "                    faces = faces.unsqueeze(0)\n",
    "                faces_iter = [f for f in faces]\n",
    "            else:\n",
    "                faces_iter = list(faces)\n",
    "\n",
    "            for face in faces_iter:\n",
    "                face_np = face.permute(1, 2, 0).cpu().numpy()\n",
    "                face_np = (face_np * 128 + 127.5).clip(0, 255).astype(np.uint8)\n",
    "                facial_images_np.append(face_np)\n",
    "\n",
    "            if len(facial_images_np) > 0:\n",
    "                emotions, scores = emotion_recognizer.predict_emotions(\n",
    "                    facial_images_np, logits=False  # í™•ë¥ ë¡œ ë°›ê¸°\n",
    "                )\n",
    "                scores = np.array(scores)  # (N, 8)\n",
    "\n",
    "                # ì²« ë²ˆì§¸ ì–¼êµ´ ê¸°ì¤€ìœ¼ë¡œ top emotion / prob ê³„ì‚°\n",
    "                probs_vec = scores[0]  # shape: (8,)\n",
    "                top_idx = int(np.argmax(probs_vec))\n",
    "                top_emotion = EMOTION_LABELS[top_idx]\n",
    "                top_prob = float(probs_vec[top_idx]) * 100.0\n",
    "\n",
    "                # í™”ë©´ì— ë°•ìŠ¤ + ê°ì • í‘œì‹œ (ëª¨ë“  ì–¼êµ´ì— ëŒ€í•´)\n",
    "                if boxes is not None:\n",
    "                    for i, box in enumerate(boxes):\n",
    "                        if box is None:\n",
    "                            continue\n",
    "                        x1, y1, x2, y2 = [int(b) for b in box]\n",
    "                        emo = emotions[i]\n",
    "                        emo_prob = float(scores[i].max()) * 100.0\n",
    "\n",
    "                        text = f\"{emo} {emo_prob:.1f}%\"\n",
    "                        cv2.rectangle(display_frame, (x1, y1), (x2, y2),\n",
    "                                      (0, 255, 0), 2)\n",
    "                        cv2.putText(display_frame, text,\n",
    "                                    (x1, max(y1 - 10, 10)),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                    0.7, (0, 255, 0), 2,\n",
    "                                    cv2.LINE_AA)\n",
    "        else:\n",
    "            cv2.putText(display_frame, \"No face detected\",\n",
    "                        (20, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        1.0, (0, 0, 255), 2,\n",
    "                        cv2.LINE_AA)\n",
    "\n",
    "        # -----------------------------\n",
    "        # C. ë…¹í™”(ë¡œê¹…) ìƒíƒœ ì²˜ë¦¬\n",
    "        # -----------------------------\n",
    "        if recording:\n",
    "            t_rel = now_time - record_start_time  # ë…¹í™” ì‹œì‘ìœ¼ë¡œë¶€í„° ê²½ê³¼ ì‹œê°„\n",
    "\n",
    "            # ê¸°ë³¸ ë¡œê·¸ í•„ë“œ\n",
    "            record = {\n",
    "                \"t\": t_rel,\n",
    "                \"fps\": fps,\n",
    "                \"top_emotion\": top_emotion,\n",
    "                \"top_prob\": top_prob,\n",
    "            }\n",
    "            for idx, emo_name in enumerate(EMOTION_LABELS):\n",
    "                record[f\"prob_{emo_name}\"] = float(probs_vec[idx]) * 100.0\n",
    "\n",
    "            # Pose ëœë“œë§ˆí¬ ê°’ ê¸°ë³¸ì€ -999ë¡œ ì±„ì›€\n",
    "            for name in LANDMARK_NAMES:\n",
    "                record[f\"{name}_x\"] = -999.0\n",
    "                record[f\"{name}_y\"] = -999.0\n",
    "                record[f\"{name}_z\"] = -999.0\n",
    "                record[f\"{name}_vis\"] = -999.0\n",
    "\n",
    "            # ì´ í”„ë ˆì„ì—ì„œ í¬ì¦ˆê°€ ë³´ì˜€ìœ¼ë©´, visibility > 0.5 ì¸ ê²ƒë§Œ ê°’ ì±„ìš°ê¸°\n",
    "            if pose_landmarks is not None:\n",
    "                for idx, name in enumerate(LANDMARK_NAMES):\n",
    "                    if idx >= len(pose_landmarks):\n",
    "                        break\n",
    "                    lm = pose_landmarks[idx]\n",
    "                    vis = float(lm.visibility) if lm.visibility is not None else 0.0\n",
    "                    if vis > 0.5:  # \"ë³´ì´ëŠ”\" ê¸°ì¤€, í•„ìš”í•˜ë©´ ì¡°ì • ê°€ëŠ¥\n",
    "                        record[f\"{name}_x\"] = float(lm.x)\n",
    "                        record[f\"{name}_y\"] = float(lm.y)\n",
    "                        record[f\"{name}_z\"] = float(lm.z)\n",
    "                        record[f\"{name}_vis\"] = vis\n",
    "                    # vis <= 0.5 ì¸ ê²½ìš°ëŠ” ê¸°ë³¸ê°’ -999 ìœ ì§€\n",
    "\n",
    "            record_buffer.append(record)\n",
    "\n",
    "            # í™”ë©´ì— REC í‘œì‹œ\n",
    "            cv2.putText(display_frame, \"REC\",\n",
    "                        (10, 70),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        1.0, (0, 0, 255), 2,\n",
    "                        cv2.LINE_AA)\n",
    "\n",
    "            # 3ì´ˆ ì§€ë‚˜ë©´ ìë™ ì¢…ë£Œ + ì €ì¥\n",
    "            if t_rel >= 3.0:\n",
    "                recording = False\n",
    "                print(f\"[INFO] Recording finished. Frames: {len(record_buffer)}\")\n",
    "                save_log_to_csv(record_buffer)\n",
    "                record_buffer = []  # ë¹„ìš°ê¸°\n",
    "\n",
    "        # -----------------------------\n",
    "        # D. FPS í‘œì‹œ\n",
    "        # -----------------------------\n",
    "        fps_text = f\"FPS: {fps:.1f}\"\n",
    "        cv2.putText(display_frame, fps_text, (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1.0, (0, 255, 255), 2,\n",
    "                    cv2.LINE_AA)\n",
    "\n",
    "        # í™”ë©´ ì¶œë ¥\n",
    "        cv2.imshow(\"Emotion + (Pose) + Logging\", display_frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "        elif key == ord(\" \"):  # SPACE ëˆ„ë¥´ë©´ 3ì´ˆ ë…¹í™” ì‹œì‘\n",
    "            if not recording:\n",
    "                recording = True\n",
    "                record_start_time = now_time\n",
    "                record_buffer = []\n",
    "                print(\"[INFO] Recording started for 3 seconds...\")\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    if USE_POSE and pose is not None:\n",
    "        pose.close()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d97411c-c508-4888-9089-7d7e9cb939fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 2 logs.\n",
      "[DONE] emotion_log_2025-11-28_17-07-54_analysis.json\n",
      "[DONE] emotion_log_2025-12-05_12-59-39_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3176\\867357523.py:20: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  nose_x = df_vis['nose_x'].rolling(window=5).mean().fillna(method='bfill')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3176\\867357523.py:21: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  nose_y = df_vis['nose_y'].rolling(window=5).mean().fillna(method='bfill')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3176\\867357523.py:20: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  nose_x = df_vis['nose_x'].rolling(window=5).mean().fillna(method='bfill')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3176\\867357523.py:21: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  nose_y = df_vis['nose_y'].rolling(window=5).mean().fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# ì„¤ì •\n",
    "LOG_DIR = \"logs\"\n",
    "OUTPUT_DIR = \"preprocessed\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def detect_head_gesture(df_vis):\n",
    "    \"\"\"\n",
    "    ì½”(nose)ì˜ ì›€ì§ì„ì„ í†µí•´ ë„ë•ì„(Nodding)ì´ë‚˜ ê°€ë¡œì €ìŒ(Shaking)ì„ ê°ì§€\n",
    "    \"\"\"\n",
    "    if df_vis['nose_vis'].mean() < 0.5:\n",
    "        return \"Not Detected\"\n",
    "\n",
    "    # ì´ë™ í‰ê· ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°\n",
    "    nose_x = df_vis['nose_x'].rolling(window=5).mean().fillna(method='bfill')\n",
    "    nose_y = df_vis['nose_y'].rolling(window=5).mean().fillna(method='bfill')\n",
    "\n",
    "    # ì›€ì§ì„ì˜ ë¶„ì‚°(Variance) ê³„ì‚° (ì›€ì§ì„ì˜ í¬ê¸°)\n",
    "    var_x = nose_x.var() * 10000  # ìŠ¤ì¼€ì¼ ë³´ì •\n",
    "    var_y = nose_y.var() * 10000\n",
    "\n",
    "    # ì›€ì§ì„ì´ ë„ˆë¬´ ì ìœ¼ë©´ Static\n",
    "    if var_x < 0.05 and var_y < 0.05:\n",
    "        return \"Static (Still)\"\n",
    "\n",
    "    # Xì¶• ì›€ì§ì„ì´ Yì¶•ë³´ë‹¤ í˜„ì €íˆ í¬ë©´ -> Shaking (No/Confusion)\n",
    "    if var_x > var_y * 1.5:\n",
    "        return \"Head Shaking (Negative/Confusion)\"\n",
    "    \n",
    "    # Yì¶• ì›€ì§ì„ì´ Xì¶•ë³´ë‹¤ í˜„ì €íˆ í¬ë©´ -> Nodding (Yes/Agreed)\n",
    "    if var_y > var_x * 1.5:\n",
    "        return \"Head Nodding (Positive/Understood)\"\n",
    "\n",
    "    return \"Dynamic (Moving)\"\n",
    "\n",
    "def analyze_posture_lean(df_vis):\n",
    "    \"\"\"\n",
    "    ì–´ê¹¨ì˜ Zì¶• ë³€í™”ë¥¼ í†µí•´ ëª¸ì„ ì•ìœ¼ë¡œ ê¸°ìš¸ì˜€ëŠ”ì§€(ì§‘ì¤‘) ë’¤ë¡œ ëºëŠ”ì§€(ì´ì™„) íŒë‹¨\n",
    "    MediaPipeì—ì„œ Zê°’ì€ ì¹´ë©”ë¼ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì‘ì•„ì§ (ìŒìˆ˜ ë°©í–¥)\n",
    "    \"\"\"\n",
    "    # ì–´ê¹¨ê°€ ë³´ì´ëŠ” í”„ë ˆì„ë§Œ\n",
    "    if df_vis['left_shoulder_vis'].mean() < 0.5:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    # ì´ˆë°˜ 30% vs í›„ë°˜ 30% Zê°’ ë¹„êµ\n",
    "    n = len(df_vis)\n",
    "    start_z = df_vis[['left_shoulder_z', 'right_shoulder_z']].iloc[:int(n*0.3)].mean().mean()\n",
    "    end_z = df_vis[['left_shoulder_z', 'right_shoulder_z']].iloc[int(n*0.7):].mean().mean()\n",
    "    \n",
    "    diff = start_z - end_z # ì–‘ìˆ˜ë©´ ë‚˜ì¤‘ì´ ë” ì‘ì•„ì§(ê°€ê¹Œì›Œì§)\n",
    "\n",
    "    # ì„ê³„ê°’ (ì‹¤í—˜ì ìœ¼ë¡œ ì¡°ì • í•„ìš”)\n",
    "    if diff > 0.05: \n",
    "        return \"Leaning Forward (High Engagement)\"\n",
    "    elif diff < -0.05:\n",
    "        return \"Leaning Backward (Relaxed/Bored)\"\n",
    "    else:\n",
    "        return \"Stable Posture\"\n",
    "\n",
    "def analyze_behavior(file_path):\n",
    "    try:\n",
    "        # -999ë¥¼ NaNìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë¡œë“œ\n",
    "        df = pd.read_csv(file_path)\n",
    "        df.replace(-999.0, np.nan, inplace=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERR] Load failed: {file_path}\")\n",
    "        return None\n",
    "\n",
    "    if len(df) < 10: return None\n",
    "\n",
    "    # 1. Pose ë°ì´í„°ê°€ ìœ íš¨í•œ í–‰ë§Œ ì¶”ì¶œ (ì½”ê°€ ë³´ì´ëŠ” í”„ë ˆì„)\n",
    "    df_vis = df.dropna(subset=['nose_x'])\n",
    "    has_pose = len(df_vis) > len(df) * 0.5  # ì „ì²´ì˜ 50% ì´ìƒ í¬ì¦ˆê°€ ì¡í˜”ëŠ”ì§€\n",
    "\n",
    "    # 2. ê°ì • ë¶„ì„ (Baseline ì œê±° ë¡œì§)\n",
    "    # Neutral, SadnessëŠ” 'ê¸°ë³¸ ìƒíƒœ'ë¡œ ê°„ì£¼. ê·¸ ì™¸ ê°ì •ì˜ ìŠ¤íŒŒì´í¬ ê°ì§€\n",
    "    emotion_cols = [c for c in df.columns if c.startswith(\"prob_\")]\n",
    "    avg_emotions = df[emotion_cols].mean()\n",
    "    \n",
    "    # ì£¼ìš” ê°ì • ì¶”ì¶œ (Neutral ì œì™¸ Top 1)\n",
    "    sorted_emotions = avg_emotions.drop(\"prob_Neutral\", errors='ignore').sort_values(ascending=False)\n",
    "    dominant_sub_emotion = sorted_emotions.index[0].replace(\"prob_\", \"\")\n",
    "    dominant_score = sorted_emotions.iloc[0]\n",
    "\n",
    "    # ê°ì • í•´ì„\n",
    "    emotion_summary = f\"Mainly Neutral, but shows traces of {dominant_sub_emotion} ({dominant_score:.1f}%)\"\n",
    "    if dominant_sub_emotion == \"Sadness\":\n",
    "        emotion_context = \"Concentrated / Serious\"\n",
    "    elif dominant_sub_emotion in [\"Anger\", \"Disgust\", \"Contempt\"]:\n",
    "        emotion_context = \"Dissatisfied / Uncomfortable\"\n",
    "    elif dominant_sub_emotion in [\"Fear\", \"Surprise\"]:\n",
    "        emotion_context = \"Confused / Surprised\"\n",
    "    elif dominant_sub_emotion == \"Happiness\":\n",
    "        emotion_context = \"Satisfied / Amused\"\n",
    "    else:\n",
    "        emotion_context = \"Passive\"\n",
    "\n",
    "    # 3. í–‰ë™(Pose) ë¶„ì„\n",
    "    gesture = \"Not Detected\"\n",
    "    posture = \"Unknown\"\n",
    "    \n",
    "    if has_pose:\n",
    "        gesture = detect_head_gesture(df_vis)\n",
    "        posture = analyze_posture_lean(df_vis)\n",
    "\n",
    "    # 4. LLMìš© ìµœì¢… ìš”ì•½ ìƒì„± (JSON)\n",
    "    result = {\n",
    "        \"metadata\": {\n",
    "            \"duration\": f\"{df['t'].max():.1f}s\",\n",
    "            \"fps\": f\"{df['fps'].mean():.1f}\"\n",
    "        },\n",
    "        \"behavior_analysis\": {\n",
    "            \"facial_expression\": emotion_context,\n",
    "            \"detailed_emotion\": emotion_summary,\n",
    "            \"head_gesture\": gesture,\n",
    "            \"body_posture\": posture\n",
    "        },\n",
    "        \"final_interpretation_for_llm\": \"\"\n",
    "    }\n",
    "\n",
    "    # 5. ì¢…í•© í•´ì„ ë¬¸ìì—´ ìƒì„± (Rule-based)\n",
    "    # ì´ ë¶€ë¶„ì´ LLM as Judgeì˜ í•µì‹¬ Inputì´ ë©ë‹ˆë‹¤.\n",
    "    interpretation = []\n",
    "    \n",
    "    if \"Forward\" in posture:\n",
    "        interpretation.append(\"User is highly engaged and leaning in to read details.\")\n",
    "    elif \"Backward\" in posture:\n",
    "        interpretation.append(\"User is sitting back, possibly bored or just skimming.\")\n",
    "        \n",
    "    if \"Nodding\" in gesture:\n",
    "        interpretation.append(\"User is nodding, indicating agreement or understanding.\")\n",
    "    elif \"Shaking\" in gesture:\n",
    "        interpretation.append(\"User is shaking head, indicating confusion or disagreement.\")\n",
    "        \n",
    "    if \"Dissatisfied\" in emotion_context:\n",
    "        interpretation.append(\"Facial expressions show signs of dissatisfaction or frustration.\")\n",
    "    elif \"Confused\" in emotion_context:\n",
    "        interpretation.append(\"User seems confused by the content.\")\n",
    "    \n",
    "    if not interpretation:\n",
    "        interpretation.append(\"User showed no significant non-verbal reaction (Passive reading).\")\n",
    "        \n",
    "    result[\"final_interpretation_for_llm\"] = \" \".join(interpretation)\n",
    "\n",
    "    return result\n",
    "\n",
    "def process_all():\n",
    "    files = glob.glob(os.path.join(LOG_DIR, \"*.csv\"))\n",
    "    print(f\"[INFO] Found {len(files)} logs.\")\n",
    "    \n",
    "    for f in files:\n",
    "        res = analyze_behavior(f)\n",
    "        if res:\n",
    "            out_name = os.path.basename(f).replace(\".csv\", \"_analysis.json\")\n",
    "            with open(os.path.join(OUTPUT_DIR, out_name), \"w\", encoding=\"utf-8\") as json_f:\n",
    "                json.dump(res, json_f, indent=4, ensure_ascii=False)\n",
    "            print(f\"[DONE] {out_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36fe38e4-8fdc-4343-98b1-f65c03c75354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 2 logs.\n",
      "[DONE] emotion_log_2025-11-28_17-07-54_analysis.json\n",
      "[DONE] emotion_log_2025-12-05_12-59-39_analysis.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# ì„¤ì •\n",
    "LOG_DIR = \"logs\"\n",
    "OUTPUT_DIR = \"preprocessed\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def detect_head_gesture(df_vis):\n",
    "    \"\"\"\n",
    "    ì½”(nose)ì˜ ì›€ì§ì„ì„ í†µí•´ ë„ë•ì„(Nodding)ì´ë‚˜ ê°€ë¡œì €ìŒ(Shaking)ì„ ê°ì§€.\n",
    "    ì œìŠ¤ì²˜ì™€ í•¨ê»˜ X/Y ë¶„ì‚° ê°’ë„ ë°˜í™˜.\n",
    "    \"\"\"\n",
    "    # ê¸°ë³¸ê°’\n",
    "    gesture = \"Not Detected\"\n",
    "    var_x = 0.0\n",
    "    var_y = 0.0\n",
    "\n",
    "    if df_vis['nose_vis'].mean() < 0.5:\n",
    "        return gesture, var_x, var_y\n",
    "\n",
    "    # ì´ë™ í‰ê· ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°\n",
    "    nose_x = df_vis['nose_x'].rolling(window=5).mean().bfill()\n",
    "    nose_y = df_vis['nose_y'].rolling(window=5).mean().bfill()\n",
    "\n",
    "    # ì›€ì§ì„ì˜ ë¶„ì‚°(Variance) ê³„ì‚° (ì›€ì§ì„ì˜ í¬ê¸°)\n",
    "    var_x = float((nose_x.var() or 0.0) * 10000)  # ìŠ¤ì¼€ì¼ ë³´ì •\n",
    "    var_y = float((nose_y.var() or 0.0) * 10000)\n",
    "\n",
    "    # ì›€ì§ì„ì´ ë„ˆë¬´ ì ìœ¼ë©´ Static\n",
    "    if var_x < 0.05 and var_y < 0.05:\n",
    "        gesture = \"Static (Still)\"\n",
    "        return gesture, var_x, var_y\n",
    "\n",
    "    # Xì¶• ì›€ì§ì„ì´ Yì¶•ë³´ë‹¤ í˜„ì €íˆ í¬ë©´ -> Shaking (No/Confusion)\n",
    "    if var_x > var_y * 1.5:\n",
    "        gesture = \"Head Shaking (Negative/Confusion)\"\n",
    "        return gesture, var_x, var_y\n",
    "    \n",
    "    # Yì¶• ì›€ì§ì„ì´ Xì¶•ë³´ë‹¤ í˜„ì €íˆ í¬ë©´ -> Nodding (Yes/Agreed)\n",
    "    if var_y > var_x * 1.5:\n",
    "        gesture = \"Head Nodding (Positive/Understood)\"\n",
    "        return gesture, var_x, var_y\n",
    "\n",
    "    gesture = \"Dynamic (Moving)\"\n",
    "    return gesture, var_x, var_y\n",
    "\n",
    "def analyze_posture_lean(df_vis):\n",
    "    \"\"\"\n",
    "    ì–´ê¹¨ì˜ Zì¶• ë³€í™”ë¥¼ í†µí•´ ëª¸ì„ ì•ìœ¼ë¡œ ê¸°ìš¸ì˜€ëŠ”ì§€(ì§‘ì¤‘) ë’¤ë¡œ ëºëŠ”ì§€(ì´ì™„) íŒë‹¨.\n",
    "    MediaPipeì—ì„œ Zê°’ì€ ì¹´ë©”ë¼ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì‘ì•„ì§ (ìŒìˆ˜ ë°©í–¥).\n",
    "    ìì„¸ì™€ í•¨ê»˜ ì‹œì‘/ë Zê°’, ì°¨ì´ë„ ë°˜í™˜.\n",
    "    \"\"\"\n",
    "    posture = \"Unknown\"\n",
    "    start_z = None\n",
    "    end_z = None\n",
    "    diff = None\n",
    "\n",
    "    # ì–´ê¹¨ê°€ ë³´ì´ëŠ” í”„ë ˆì„ë§Œ\n",
    "    if df_vis['left_shoulder_vis'].mean() < 0.5:\n",
    "        return posture, start_z, end_z, diff\n",
    "\n",
    "    n = len(df_vis)\n",
    "    if n < 5:\n",
    "        return posture, start_z, end_z, diff\n",
    "\n",
    "    # ì´ˆë°˜ 30% vs í›„ë°˜ 30% Zê°’ ë¹„êµ\n",
    "    start_z = df_vis[['left_shoulder_z', 'right_shoulder_z']].iloc[:int(n*0.3)].mean().mean()\n",
    "    end_z = df_vis[['left_shoulder_z', 'right_shoulder_z']].iloc[int(n*0.7):].mean().mean()\n",
    "    start_z = float(start_z)\n",
    "    end_z = float(end_z)\n",
    "\n",
    "    diff = start_z - end_z # ì–‘ìˆ˜ë©´ ë‚˜ì¤‘ì´ ë” ì‘ì•„ì§(ê°€ê¹Œì›Œì§)\n",
    "\n",
    "    # ì„ê³„ê°’ (ì‹¤í—˜ì ìœ¼ë¡œ ì¡°ì • í•„ìš”)\n",
    "    if diff > 0.05: \n",
    "        posture = \"Leaning Forward (High Engagement)\"\n",
    "    elif diff < -0.05:\n",
    "        posture = \"Leaning Backward (Relaxed/Bored)\"\n",
    "    else:\n",
    "        posture = \"Stable Posture\"\n",
    "\n",
    "    return posture, start_z, end_z, float(diff)\n",
    "\n",
    "def analyze_behavior(file_path):\n",
    "    try:\n",
    "        # -999ë¥¼ NaNìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë¡œë“œ\n",
    "        df = pd.read_csv(file_path)\n",
    "        df.replace(-999.0, np.nan, inplace=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERR] Load failed: {file_path} ({e})\")\n",
    "        return None\n",
    "\n",
    "    if len(df) < 10:\n",
    "        return None\n",
    "\n",
    "    # 1. Pose ë°ì´í„°ê°€ ìœ íš¨í•œ í–‰ë§Œ ì¶”ì¶œ (ì½”ê°€ ë³´ì´ëŠ” í”„ë ˆì„)\n",
    "    df_vis = df.dropna(subset=['nose_x'])\n",
    "    has_pose = len(df_vis) > len(df) * 0.5  # ì „ì²´ì˜ 50% ì´ìƒ í¬ì¦ˆê°€ ì¡í˜”ëŠ”ì§€\n",
    "\n",
    "    # 2. ê°ì • ë¶„ì„ (Baseline ì œê±° ë¡œì§)\n",
    "    # Neutral, SadnessëŠ” 'ê¸°ë³¸ ìƒíƒœ'ë¡œ ê°„ì£¼. ê·¸ ì™¸ ê°ì •ì˜ ìŠ¤íŒŒì´í¬ ê°ì§€\n",
    "    emotion_cols = [c for c in df.columns if c.startswith(\"prob_\")]\n",
    "    avg_emotions = df[emotion_cols].mean()\n",
    "\n",
    "    # ì£¼ìš” ê°ì • ì¶”ì¶œ (Neutral ì œì™¸ Top 1)\n",
    "    sorted_emotions = avg_emotions.drop(\"prob_Neutral\", errors='ignore').sort_values(ascending=False)\n",
    "    dominant_sub_emotion = sorted_emotions.index[0].replace(\"prob_\", \"\") if len(sorted_emotions) > 0 else \"Unknown\"\n",
    "    dominant_score = sorted_emotions.iloc[0] if len(sorted_emotions) > 0 else 0.0\n",
    "\n",
    "    # ê°ì • í•´ì„\n",
    "    emotion_summary = f\"Mainly Neutral, but shows traces of {dominant_sub_emotion} ({dominant_score:.1f}%)\"\n",
    "    if dominant_sub_emotion == \"Sadness\":\n",
    "        emotion_context = \"Concentrated / Serious\"\n",
    "    elif dominant_sub_emotion in [\"Anger\", \"Disgust\", \"Contempt\"]:\n",
    "        emotion_context = \"Dissatisfied / Uncomfortable\"\n",
    "    elif dominant_sub_emotion in [\"Fear\", \"Surprise\"]:\n",
    "        emotion_context = \"Confused / Surprised\"\n",
    "    elif dominant_sub_emotion == \"Happiness\":\n",
    "        emotion_context = \"Satisfied / Amused\"\n",
    "    else:\n",
    "        emotion_context = \"Passive\"\n",
    "\n",
    "    # 3. í–‰ë™(Pose) ë¶„ì„ + í†µê³„\n",
    "    gesture = \"Not Detected\"\n",
    "    posture = \"Unknown\"\n",
    "    nose_var_x = 0.0\n",
    "    nose_var_y = 0.0\n",
    "    shoulder_start_z = None\n",
    "    shoulder_end_z = None\n",
    "    shoulder_z_diff = None\n",
    "    \n",
    "    if has_pose:\n",
    "        gesture, nose_var_x, nose_var_y = detect_head_gesture(df_vis)\n",
    "        posture, shoulder_start_z, shoulder_end_z, shoulder_z_diff = analyze_posture_lean(df_vis)\n",
    "\n",
    "    # 4. LLMìš© ìµœì¢… ìš”ì•½ ìƒì„± (JSON)\n",
    "\n",
    "    # ë©”íƒ€ë°ì´í„°: ë¬¸ìì—´ + ìˆ«ì ë²„ì „ í•¨ê»˜\n",
    "    duration_sec = float(df['t'].max())\n",
    "    fps_mean = float(df['fps'].mean())\n",
    "    metadata = {\n",
    "        \"duration\": f\"{duration_sec:.1f}s\",\n",
    "        \"fps\": f\"{fps_mean:.1f}\",\n",
    "        \"duration_sec\": duration_sec,\n",
    "        \"fps_mean\": fps_mean,\n",
    "        \"source_file\": os.path.basename(file_path)\n",
    "    }\n",
    "\n",
    "    # README ë„ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ (ì§€ê¸ˆì€ placeholder, ë‚˜ì¤‘ì— íŒŒì¼ëª…/ì™¸ë¶€ ì •ë³´ë¡œ ì±„ì›Œë„ ë¨)\n",
    "    context = {\n",
    "        \"readme_version\": \"unknown\",               # TODO: í•„ìš”ì‹œ ì™¸ë¶€ì—ì„œ ì£¼ì…\n",
    "        \"section\": \"unknown\",                     # TODO: ì˜ˆ: \"Installation\", \"Getting Started\"\n",
    "        \"user_id\": None,                          # ìˆìœ¼ë©´ ì±„ìš°ê¸°\n",
    "        \"window_start_sec\": float(df[\"t\"].min()),\n",
    "        \"window_end_sec\": float(df[\"t\"].max())\n",
    "    }\n",
    "\n",
    "    # ê°ì • í†µê³„ (í‰ê·  ê°’ë§Œ ìš°ì„  ì €ì¥, í•„ìš”ì‹œ max/std ì¶”ê°€ ê°€ëŠ¥)\n",
    "    emotion_stats = {\n",
    "        \"mean\": {col: float(avg_emotions[col]) for col in emotion_cols}\n",
    "    }\n",
    "\n",
    "    # í¬ì¦ˆ í†µê³„\n",
    "    pose_stats = {\n",
    "        \"has_pose\": bool(has_pose),\n",
    "        \"nose_var_x\": float(nose_var_x),\n",
    "        \"nose_var_y\": float(nose_var_y),\n",
    "        \"shoulder_start_z\": float(shoulder_start_z) if shoulder_start_z is not None else None,\n",
    "        \"shoulder_end_z\": float(shoulder_end_z) if shoulder_end_z is not None else None,\n",
    "        \"shoulder_z_diff\": float(shoulder_z_diff) if shoulder_z_diff is not None else None\n",
    "    }\n",
    "\n",
    "    behavior_analysis = {\n",
    "        \"facial_expression\": emotion_context,\n",
    "        \"detailed_emotion\": emotion_summary,\n",
    "        \"head_gesture\": gesture,\n",
    "        \"body_posture\": posture\n",
    "    }\n",
    "\n",
    "    result = {\n",
    "        \"metadata\": metadata,\n",
    "        \"context\": context,\n",
    "        \"emotion_stats\": emotion_stats,\n",
    "        \"pose_stats\": pose_stats,\n",
    "        \"behavior_analysis\": behavior_analysis,\n",
    "        \"final_interpretation_for_llm\": \"\"\n",
    "    }\n",
    "\n",
    "    # 5. ì¢…í•© í•´ì„ ë¬¸ìì—´ ìƒì„± (Rule-based)\n",
    "    # ì´ ë¶€ë¶„ì´ LLM as Judgeì˜ í•µì‹¬ Inputì´ ë©ë‹ˆë‹¤.\n",
    "    interpretation = []\n",
    "    \n",
    "    if \"Forward\" in posture:\n",
    "        interpretation.append(\"User is highly engaged and leaning in to read details.\")\n",
    "    elif \"Backward\" in posture:\n",
    "        interpretation.append(\"User is sitting back, possibly bored or just skimming.\")\n",
    "        \n",
    "    if \"Nodding\" in gesture:\n",
    "        interpretation.append(\"User is nodding, indicating agreement or understanding.\")\n",
    "    elif \"Shaking\" in gesture:\n",
    "        interpretation.append(\"User is shaking head, indicating confusion or disagreement.\")\n",
    "        \n",
    "    if \"Dissatisfied\" in emotion_context:\n",
    "        interpretation.append(\"Facial expressions show signs of dissatisfaction or frustration.\")\n",
    "    elif \"Confused\" in emotion_context:\n",
    "        interpretation.append(\"User seems confused by the content.\")\n",
    "        \n",
    "    if not interpretation:\n",
    "        interpretation.append(\"User showed no significant non-verbal reaction (Passive reading).\")\n",
    "        \n",
    "    result[\"final_interpretation_for_llm\"] = \" \".join(interpretation)\n",
    "\n",
    "    return result\n",
    "\n",
    "def process_all():\n",
    "    files = glob.glob(os.path.join(LOG_DIR, \"*.csv\"))\n",
    "    # âœ… ì´ë ‡ê²Œ ê³ ì³ì•¼ í•¨ (f-string)\n",
    "    print(f\"[INFO] Found {len(files)} logs.\")\n",
    "    \n",
    "    for f in files:\n",
    "        res = analyze_behavior(f)\n",
    "        if res:\n",
    "            out_name = os.path.basename(f).replace(\".csv\", \"_analysis.json\")\n",
    "            with open(os.path.join(OUTPUT_DIR, out_name), \"w\", encoding=\"utf-8\") as json_f:\n",
    "                json.dump(res, json_f, indent=4, ensure_ascii=False)\n",
    "            print(f\"[DONE] {out_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d68a90e-8214-4493-8dc2-dde0cca5c721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb0bc166-6bed-41fb-a8ea-594ebe4c1281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 2 analysis jsons\n",
      "[DONE] created guideline_seeds\\seed_001_None_unknown_unknown.json\n",
      "[DONE] created guideline_seeds\\seed_002_None_unknown_unknown.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "PREPROCESSED_DIR = \"preprocessed\"\n",
    "README_DIR = \"readmes\"          # readme_v1.md, readme_v2.md ...\n",
    "SEED_DIR = \"guideline_seeds\"\n",
    "os.makedirs(SEED_DIR, exist_ok=True)\n",
    "\n",
    "def load_readme(readme_version: str) -> str:\n",
    "    path = os.path.join(README_DIR, f\"readme_{readme_version}.md\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    return \"\"\n",
    "\n",
    "def main():\n",
    "    files = glob.glob(os.path.join(PREPROCESSED_DIR, \"*_analysis.json\"))\n",
    "    print(\"[INFO] Found\", len(files), \"analysis jsons\")\n",
    "\n",
    "    for i, path in enumerate(sorted(files), start=1):\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            behavior = json.load(f)\n",
    "\n",
    "        ctx = behavior.get(\"context\", {})\n",
    "        readme_version = ctx.get(\"readme_version\", \"unknown\")\n",
    "        section = ctx.get(\"section\", \"unknown\")\n",
    "        user_id = ctx.get(\"user_id\", \"unknown\")\n",
    "\n",
    "        readme_text = load_readme(readme_version)\n",
    "\n",
    "        seed = {\n",
    "            \"meta\": {\n",
    "                \"source_analysis_file\": os.path.basename(path),\n",
    "                \"user_id\": user_id,\n",
    "                \"readme_version\": readme_version,\n",
    "                \"section\": section\n",
    "            },\n",
    "            \"behavior\": behavior,\n",
    "            \"readme_text\": readme_text,\n",
    "            \"expert_comment\": \"TODO: ì „ë¬¸ê°€ê°€ ì—¬ê¸° ë‚´ìš©ì„ ì±„ì›Œì£¼ì„¸ìš”.\"\n",
    "        }\n",
    "\n",
    "        out_name = f\"seed_{i:03d}_{user_id}_{readme_version}_{section}.json\"\n",
    "        out_path = os.path.join(SEED_DIR, out_name)\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(seed, f, ensure_ascii=False, indent=2)\n",
    "        print(\"[DONE] created\", out_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1023f2fe-fe7c-4cef-92fa-f8c371e2de6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your GEMINI_API_KEY:  .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIê°€ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.\n",
      "âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì—ëŸ¬ ë°œìƒ: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"API key not valid. Please pass a valid API key.\"\n",
      "]\n",
      "ë¶„ì„ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "import google.generativeai as genai\n",
    "\n",
    "# 1) ì„¤ì •: ì‹¤í–‰ ì‹œ API í‚¤ ì…ë ¥\n",
    "def configure_gemini():\n",
    "    # ë§¤ë²ˆ ì…ë ¥í•˜ê¸° ê·€ì°®ìœ¼ì‹œë©´ ì•„ë˜ \"\" ì•ˆì— í‚¤ë¥¼ ì§ì ‘ ë„£ìœ¼ì…”ë„ ë©ë‹ˆë‹¤.\n",
    "    api_key_input = input(\"Enter your GEMINI_API_KEY: \").strip()\n",
    "    if not api_key_input:\n",
    "        # ì…ë ¥ì´ ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ë‚˜ í•˜ë“œì½”ë”©ëœ í‚¤ë¥¼ ì“´ë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜ ì—ëŸ¬ ì²˜ë¦¬\n",
    "        raise ValueError(\"API Keyê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    genai.configure(api_key=api_key_input)\n",
    "\n",
    "# 2) ê°œì„ ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì˜ì‚¬ê²°ì • ìµœì í™” + í•œêµ­ì–´ ê°•ì œ)\n",
    "OPTION_EXPLAIN_SYSTEM_PROMPT = \"\"\"\n",
    "You are a wise 'Decision Consultant' AI. Your goal is to help a user compare multiple options and make the best choice.\n",
    "\n",
    "TASKS:\n",
    "1. Analyze each option provided by the user.\n",
    "2. Provide a structured analysis in KOREAN.\n",
    "3. Be objective but insightful. Highlights the unique trade-offs.\n",
    "\n",
    "OUTPUT FORMAT (JSON ONLY):\n",
    "You must output a single valid JSON object. Do not include markdown formatting (like ```json).\n",
    "Structure:\n",
    "{\n",
    "  \"options\": [\n",
    "    {\n",
    "      \"id\": \"opt1\",\n",
    "      \"original_text\": \"<String: Original Text>\",\n",
    "      \"title\": \"<String: A catchy, short title in Korean>\",\n",
    "      \"summary\": \"<String: 2 sentences explaining what this is>\",\n",
    "      \"pros\": [\"<String>\", \"<String>\", \"<String>\"],\n",
    "      \"cons\": [\"<String>\", \"<String>\", \"<String>\"],\n",
    "      \"fit_for\": \"<String: Type of person or mood this suits best>\",\n",
    "      \"rating\": <Integer: 1 to 5 score based on general appeal>\n",
    "    },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "\n",
    "RULES:\n",
    "- All values (title, summary, pros, cons, fit_for) MUST be in KOREAN.\n",
    "- 'pros' and 'cons' should be specific, not generic.\n",
    "- 'rating' is your subjective recommendation score (1=Bad, 5=Excellent).\n",
    "\"\"\"\n",
    "\n",
    "OPTION_EXPLAIN_USER_TEMPLATE = \"\"\"\n",
    "ì‚¬ìš©ìê°€ ê³ ë¯¼ ì¤‘ì¸ ë‹¤ìŒ ì„ íƒì§€ë“¤ì„ ë¶„ì„í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "[ì„ íƒì§€ ëª©ë¡]\n",
    "{options_block}\n",
    "\n",
    "ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "\"\"\"\n",
    "\n",
    "def build_options_block(options: List[str]) -> str:\n",
    "    lines = []\n",
    "    for i, opt in enumerate(options, start=1):\n",
    "        lines.append(f\"{i}. {opt}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def explain_options(options: List[str], model_name: str = \"gemini-2.5-flash\") -> Dict[str, Any]:\n",
    "    if not options:\n",
    "        raise ValueError(\"ì„ íƒì§€ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    options_block = build_options_block(options)\n",
    "    user_prompt = OPTION_EXPLAIN_USER_TEMPLATE.format(options_block=options_block)\n",
    "\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=model_name,\n",
    "        system_instruction=OPTION_EXPLAIN_SYSTEM_PROMPT,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # [ì¤‘ìš” ë³€ê²½ì  1] max_output_tokensë¥¼ 4096ìœ¼ë¡œ ëŒ€í­ ìƒí–¥ (ì˜ë¦¼ ë°©ì§€)\n",
    "        response = model.generate_content(\n",
    "            user_prompt,\n",
    "            generation_config={\n",
    "                \"temperature\": 0.4,\n",
    "                \"max_output_tokens\": 4096, \n",
    "                \"response_mime_type\": \"application/json\",\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        text = response.text.strip()\n",
    "\n",
    "        # [ì¤‘ìš” ë³€ê²½ì  2] ë” ì•ˆì „í•œ ë§ˆí¬ë‹¤ìš´ ì œê±° ë¡œì§\n",
    "        # ```json ê³¼ ``` ë¥¼ ì‹¤ìˆ˜ë¡œ í¬í•¨í–ˆì„ ê²½ìš° ì œê±°\n",
    "        if text.startswith(\"```\"):\n",
    "            text = re.sub(r\"^```json\\s*|^```\\s*|```\\s*$\", \"\", text, flags=re.MULTILINE).strip()\n",
    "\n",
    "        data = json.loads(text)\n",
    "        return data\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"âŒ JSON íŒŒì‹± ì—ëŸ¬ ë°œìƒ: {e}\")\n",
    "        print(\"--- ëª¨ë¸ì´ ë°˜í™˜í•œ ì›ë³¸ í…ìŠ¤íŠ¸ (ì•ë¶€ë¶„ 500ì) ---\")\n",
    "        print(response.text[:500] + \"...\") \n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì—ëŸ¬ ë°œìƒ: {e}\")\n",
    "        return {}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. ì„¤ì •\n",
    "    configure_gemini()\n",
    "\n",
    "    # 2. ê³ ë¯¼ ì¤‘ì¸ ì„ íƒì§€\n",
    "    user_options = [\n",
    "        \"ì œì£¼ë„ 2ë°• 3ì¼ ì—¬í–‰ (ë¹„í–‰ê¸°ê°’ ë¹„ìŒˆ)\",\n",
    "        \"ë¶€ì‚° 1ë°• 2ì¼ KTX ë¨¹ë°© ì—¬í–‰\",\n",
    "        \"ì§‘ì—ì„œ 3ì¼ ë™ì•ˆ ë„·í”Œë¦­ìŠ¤ ë³´ë©° ë°°ë‹¬ìŒì‹ ì‹œì¼œë¨¹ê¸°\",\n",
    "    ]\n",
    "\n",
    "    print(\"AIê°€ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.\")\n",
    "\n",
    "    # 3. ë¶„ì„ ê²°ê³¼ ë°›ê¸°\n",
    "    result = explain_options(user_options)\n",
    "\n",
    "    # 4. ì˜ˆì˜ê²Œ ì¶œë ¥\n",
    "    if result and \"options\" in result:\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        for item in result[\"options\"]:\n",
    "            print(f\"[{item['title']}] (â­ {item['rating']}/5)\")\n",
    "            print(f\"ğŸ“„ ìš”ì•½: {item['summary']}\")\n",
    "            print(f\"ğŸ‘ ì¥ì : {', '.join(item['pros'])}\")\n",
    "            print(f\"ğŸ‘ ë‹¨ì : {', '.join(item['cons'])}\")\n",
    "            print(f\"ğŸ‘¤ ì¶”ì²œ: {item['fit_for']}\")\n",
    "            print(\"-\" * 40)\n",
    "    else:\n",
    "        print(\"ë¶„ì„ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad2acf80-c93f-4ee7-b8a5-5880d7c6a749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 2 seeds to analyze.\n",
      "\n",
      "ğŸ” Analyzing: seed_001_None_unknown_unknown.json\n",
      "ğŸ§  Reasoning: The analysis of the user's behavior reveals a mix of conflicting signals. \n",
      "1.  **Engagement (Posture)**: The user's 'Leaning Backward' posture suggests low cognitive engagement or passive reading, indicating a lower level of interest.\n",
      "2.  **Agreement vs. Confusion (Head Gestures)**: The 'Head Nodding' is a strong positive signal, indicating agreement or understanding of the content.\n",
      "3.  **Emotional Resonance**: The emotional data shows a baseline of 'Neutral' (57.26%), with 'trace amounts' of 'Happiness' (7.84%) and 'Surprise' (1.97%) which are positive indicators. 'Sadness' (16.31%) is also present, which, according to guidelines, suggests deep concentration or seriousness rather than unhappiness in this context. \n",
      "4.  **Crucially**, 'Contempt' is present at a significant level (14.22%). According to the guidelines, 'Contempt' strongly suggests dissatisfaction (a negative signal).\n",
      "\n",
      "While the user appears to understand or agree with aspects of the content ('Nodding') and shows some mild positive emotion and concentration, the significant presence of 'Contempt' (dissatisfaction) combined with a 'Leaning Backward' posture (low engagement/interest) prevents a conclusion of overall positive preference. The strong conflicting signals, particularly the high level of contempt against the nodding, lead to a 'Neutral' verdict, indicating that understanding does not equate to positive subconscious preference in this instance.\n",
      "ğŸ“Š Verdict: Neutral (Interest: Low)\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ” Analyzing: seed_002_None_unknown_unknown.json\n",
      "ğŸ§  Reasoning: The user exhibited 'Leaning Forward' posture, which signifies high cognitive engagement and interest in the content. However, this engagement was accompanied by 'Head Shaking,' a strong negative signal indicating confusion, disagreement, or rejection. Emotionally, while 'Neutral' was dominant, there was a noticeable presence of 'Sadness' (22.3%), interpreted as deep concentration, and 'Anger' (8.17%), suggesting dissatisfaction. The very low percentage of 'Happiness' (0.27%) means there was no significant positive emotional resonance. Despite high engagement, the prominent negative signals (head shaking, anger) indicate an active negative processing of the content.\n",
      "ğŸ“Š Verdict: Negative (Interest: Low)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# 2. CLONE ê¸°ë°˜ í–‰ë™ ì‹¬ë¦¬í•™ì í˜ë¥´ì†Œë‚˜ ì •ì˜\n",
    "BEHAVIOR_ANALYST_SYSTEM_PROMPT = \"\"\"\n",
    "### Role\n",
    "You are an expert 'Behavioral Psychologist' employing the CLONE (Clinical Reasoning via Neuropsychologist Emulation) methodology.\n",
    "Your goal is to analyze non-verbal behavioral data to infer a user's subconscious preference toward a specific option.\n",
    "\n",
    "### Diagnostic Guidelines (Synthesis of Behavioral Signals)\n",
    "Follow these rules to interpret the data:\n",
    "\n",
    "1. **Engagement (Posture & Gaze)**:\n",
    "   - \"Leaning Forward\" indicates high cognitive engagement and interest.\n",
    "   - \"Leaning Backward\" often indicates relaxation, low interest, or passive reading.\n",
    "   \n",
    "2. **Agreement vs. Confusion (Head Gestures)**:\n",
    "   - \"Nodding\" strongly correlates with agreement or understanding (Positive Signal).\n",
    "   - \"Shaking\" correlates with disagreement, confusion, or rejection (Negative Signal).\n",
    "   \n",
    "3. **Emotional Resonance**:\n",
    "   - Trace amounts of \"Happiness\" or \"Surprise\" amidst a Neutral baseline suggest positive interest.\n",
    "   - \"Anger\", \"Disgust\", or \"Contempt\" suggest dissatisfaction (Negative Signal).\n",
    "   - \"Sadness\" in this context often indicates deep concentration/seriousness, not necessarily unhappiness.\n",
    "\n",
    "### Task\n",
    "Analyze the provided JSON data containing behavioral statistics and rule-based summaries.\n",
    "Determine if the user showed a POSITIVE, NEGATIVE, or NEUTRAL reaction to the content they were viewing.\n",
    "\n",
    "### Output Format (JSON)\n",
    "{\n",
    "  \"rationale\": \"Step-by-step reasoning based on the guidelines...\",\n",
    "  \"predicted_interest_level\": \"High/Medium/Low\",\n",
    "  \"key_signals\": [\"Leaning Forward\", \"Micro-expression of Happiness\", ...],\n",
    "  \"final_verdict\": \"Positive/Negative/Neutral\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def analyze_user_preference_with_llm(seed_file_path):\n",
    "    with open(seed_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        seed_data = json.load(f)\n",
    "\n",
    "    # LLMì—ê²Œ ì¤„ ì…ë ¥ í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "    user_content_str = json.dumps(seed_data['behavior'], indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ë‚´ìš© (ì‚¬ìš©ìê°€ ë­˜ ë³´ê³  ìˆì—ˆëŠ”ì§€)\n",
    "    # í˜„ì¬ëŠ” readme_textë¡œ ë˜ì–´ìˆì§€ë§Œ, ì¶”í›„ option_descriptionìœ¼ë¡œ ë°”ë€Œì–´ì•¼ í•¨\n",
    "    viewed_content = seed_data.get(\"readme_text\", \"Content Info Missing\") \n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    ### Context\n",
    "    The user was reading the following content:\n",
    "    ---\n",
    "    {viewed_content[:500]}... (truncated)\n",
    "    ---\n",
    "\n",
    "    ### Observed Behavioral Data\n",
    "    {user_content_str}\n",
    "\n",
    "    Based on the 'Diagnostic Guidelines', analyze the user's subconscious reaction.\n",
    "    \"\"\"\n",
    "\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.5-flash\",\n",
    "        system_instruction=BEHAVIOR_ANALYST_SYSTEM_PROMPT\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config={\"response_mime_type\": \"application/json\"}\n",
    "        )\n",
    "        return json.loads(response.text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {seed_file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- ì‹¤í–‰ë¶€ ---\n",
    "if __name__ == \"__main__\":\n",
    "    seed_files = glob.glob(os.path.join(\"guideline_seeds\", \"*.json\"))\n",
    "    print(f\"[INFO] Found {len(seed_files)} seeds to analyze.\")\n",
    "\n",
    "    for seed_path in seed_files:\n",
    "        print(f\"\\nğŸ” Analyzing: {os.path.basename(seed_path)}\")\n",
    "        result = analyze_user_preference_with_llm(seed_path)\n",
    "        \n",
    "        if result:\n",
    "            print(f\"ğŸ§  Reasoning: {result.get('rationale')}\")\n",
    "            print(f\"ğŸ“Š Verdict: {result.get('final_verdict')} (Interest: {result.get('predicted_interest_level')})\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe3bff9-9ad5-4105-afd5-ab1fa67667dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (OSS)",
   "language": "python",
   "name": "oss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
